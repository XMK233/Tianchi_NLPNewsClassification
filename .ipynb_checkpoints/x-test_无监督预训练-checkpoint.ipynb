{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ac0a46f2-b13a-4d74-88ae-413863bc3989",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import BertModel, BertTokenizer, BertConfig\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "35dabc28-e733-4984-a00e-4d08ce05ef2b",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'type_of_class' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 8\u001b[0m\n\u001b[1;32m      1\u001b[0m BertModel(BertConfig(\n\u001b[1;32m      2\u001b[0m                 vocab_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1000\u001b[39m,\n\u001b[1;32m      3\u001b[0m                 hidden_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m32\u001b[39m,\n\u001b[1;32m      4\u001b[0m                 num_hidden_layers\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m4\u001b[39m,\n\u001b[1;32m      5\u001b[0m                 num_attention_heads\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m,\n\u001b[1;32m      6\u001b[0m                 intermediate_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m64\u001b[39m,\n\u001b[1;32m      7\u001b[0m                 max_position_embeddings\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m,\n\u001b[0;32m----> 8\u001b[0m                 num_labels\u001b[38;5;241m=\u001b[39m\u001b[43mtype_of_class\u001b[49m\n\u001b[1;32m      9\u001b[0m             ))\n",
      "\u001b[0;31mNameError\u001b[0m: name 'type_of_class' is not defined"
     ]
    }
   ],
   "source": [
    "BertModel(BertConfig(\n",
    "                vocab_size=1000,\n",
    "                hidden_size=32,\n",
    "                num_hidden_layers=4,\n",
    "                num_attention_heads=2,\n",
    "                intermediate_size=64,\n",
    "                max_position_embeddings=5,\n",
    "                num_labels=type_of_class\n",
    "            ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3490566-f14c-42bb-8703-c2a4ee250d58",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82eb967c-6989-44d4-8a33-4d85774e0d82",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e1a4e7b-2f2f-464e-a9bd-175652f196a2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b2579ed-7f27-4bdd-b275-4c9b9cf2e4b6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ecc38e1-d21b-49ea-930a-51ed306dc33a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# 定义联合训练模型\n",
    "class JointModel(nn.Module):\n",
    "    def __init__(self, bert_model_name, num_labels, mlm_vocab_size=30522):\n",
    "        super().__init__()\n",
    "        # 共享的BERT编码器\n",
    "        self.bert = BertModel.from_pretrained(bert_model_name)\n",
    "        self.config = self.bert.config\n",
    "        \n",
    "        # 分类任务头\n",
    "        self.classifier = nn.Linear(self.config.hidden_size, num_labels)\n",
    "        \n",
    "        # MLM任务头\n",
    "        self.mlm_head = nn.Linear(self.config.hidden_size, mlm_vocab_size)\n",
    "        \n",
    "        # SOP任务头\n",
    "        self.sop_head = nn.Linear(self.config.hidden_size, 2)  # 二分类：顺序是否正确\n",
    "\n",
    "    def forward(self, input_ids, attention_mask, token_type_ids=None, \n",
    "                mlm_labels=None, sop_labels=None):\n",
    "        # 共享编码器输出\n",
    "        outputs = self.bert(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            token_type_ids=token_type_ids,\n",
    "            return_dict=True\n",
    "        )\n",
    "        sequence_output = outputs.last_hidden_state  # [batch, seq_len, hidden]\n",
    "        pooled_output = outputs.pooler_output        # [batch, hidden]\n",
    "\n",
    "        # 分类任务\n",
    "        cls_logits = self.classifier(pooled_output)  # [batch, num_labels]\n",
    "\n",
    "        # MLM任务\n",
    "        mlm_logits = self.mlm_head(sequence_output)  # [batch, seq_len, vocab]\n",
    "\n",
    "        # SOP任务\n",
    "        sop_logits = self.sop_head(pooled_output)    # [batch, 2]\n",
    "\n",
    "        # 计算各任务损失\n",
    "        losses = {}\n",
    "        if mlm_labels is not None:\n",
    "            loss_fct = nn.CrossEntropyLoss(ignore_index=-100)\n",
    "            mlm_loss = loss_fct(\n",
    "                mlm_logits.view(-1, self.config.vocab_size),\n",
    "                mlm_labels.view(-1)\n",
    "            )\n",
    "            losses[\"mlm\"] = mlm_loss\n",
    "\n",
    "        if sop_labels is not None:\n",
    "            loss_fct = nn.CrossEntropyLoss()\n",
    "            sop_loss = loss_fct(sop_logits.view(-1, 2), sop_labels.view(-1))\n",
    "            losses[\"sop\"] = sop_loss\n",
    "\n",
    "        return cls_logits, losses\n",
    "\n",
    "# 自定义数据集\n",
    "class JointDataset(Dataset):\n",
    "    def __init__(self, texts, labels, tokenizer, max_len=128, mlm_prob=0.15):\n",
    "        \"\"\"\n",
    "        texts: 原始文本列表\n",
    "        labels: 分类标签\n",
    "        tokenizer: BERT tokenizer\n",
    "        mlm_prob: 随机mask概率\n",
    "        \"\"\"\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "        self.mlm_prob = mlm_prob\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = self.texts[idx]\n",
    "        label = self.labels[idx]\n",
    "\n",
    "        # 1. 原始文本编码（用于分类）\n",
    "        encoding = self.tokenizer(\n",
    "            text,\n",
    "            max_length=self.max_len,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "\n",
    "        # 2. 生成MLM数据\n",
    "        input_ids = encoding[\"input_ids\"].squeeze(0)\n",
    "        mlm_input_ids = input_ids.clone()\n",
    "        mlm_labels = torch.full_like(input_ids, -100)  # 默认忽略非mask位置\n",
    "        \n",
    "        # 随机选择15%的token进行mask\n",
    "        rand = torch.rand(input_ids.shape)\n",
    "        mask_indices = (rand < self.mlm_prob) & (input_ids != self.tokenizer.cls_token_id) & (input_ids != self.tokenizer.sep_token_id)\n",
    "        \n",
    "        # 80%替换为[MASK], 10%随机词, 10%保持原词\n",
    "        replace_mask = mask_indices & (torch.rand(mask_indices.shape) < 0.8)\n",
    "        random_mask = mask_indices & (torch.rand(mask_indices.shape) < 0.5) & ~replace_mask\n",
    "        \n",
    "        mlm_input_ids[replace_mask] = self.tokenizer.mask_token_id\n",
    "        mlm_input_ids[random_mask] = torch.randint(0, self.tokenizer.vocab_size, (sum(random_mask),))\n",
    "        mlm_labels[mask_indices] = input_ids[mask_indices]\n",
    "\n",
    "        # 3. 生成SOP数据（句子顺序预测）\n",
    "        sentences = text.split('.')  # 简单按句号分割\n",
    "        if len(sentences) >= 2:\n",
    "            # 50%概率交换前两句\n",
    "            if torch.rand(1) < 0.5:\n",
    "                sent1, sent2 = sentences[0], sentences[1]\n",
    "                sop_label = 1  # 顺序正确\n",
    "            else:\n",
    "                sent1, sent2 = sentences[1], sentences[0]\n",
    "                sop_label = 0  # 顺序错误\n",
    "            \n",
    "            sop_text = f\"{sent1} [SEP] {sent2}\"\n",
    "            sop_encoding = self.tokenizer(\n",
    "                sop_text,\n",
    "                max_length=self.max_len,\n",
    "                padding='max_length',\n",
    "                truncation=True,\n",
    "                return_tensors='pt'\n",
    "            )\n",
    "            sop_input_ids = sop_encoding[\"input_ids\"].squeeze(0)\n",
    "            sop_attention_mask = sop_encoding[\"attention_mask\"].squeeze(0)\n",
    "        else:\n",
    "            sop_input_ids = input_ids\n",
    "            sop_attention_mask = encoding[\"attention_mask\"].squeeze(0)\n",
    "            sop_label = -100  # 忽略此样本\n",
    "\n",
    "        return {\n",
    "            \"input_ids\": input_ids,\n",
    "            \"attention_mask\": encoding[\"attention_mask\"].squeeze(0),\n",
    "            \"labels\": torch.tensor(label, dtype=torch.long),\n",
    "            \"mlm_input_ids\": mlm_input_ids,\n",
    "            \"mlm_labels\": mlm_labels,\n",
    "            \"sop_input_ids\": sop_input_ids,\n",
    "            \"sop_attention_mask\": sop_attention_mask,\n",
    "            \"sop_labels\": torch.tensor(sop_label, dtype=torch.long)\n",
    "        }\n",
    "\n",
    "# 训练循环示例\n",
    "def train_epoch(model, dataloader, optimizer, device, loss_weights={'cls': 1.0, 'mlm': 0.5, 'sop': 0.5}):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    \n",
    "    for batch in dataloader:\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # 分类任务\n",
    "        cls_output, _ = model(\n",
    "            input_ids=batch['input_ids'].to(device),\n",
    "            attention_mask=batch['attention_mask'].to(device)\n",
    "        )\n",
    "        cls_loss = nn.CrossEntropyLoss()(cls_output, batch['labels'].to(device))\n",
    "        \n",
    "        # MLM任务\n",
    "        _, mlm_losses = model(\n",
    "            input_ids=batch['mlm_input_ids'].to(device),\n",
    "            attention_mask=batch['attention_mask'].to(device),\n",
    "            mlm_labels=batch['mlm_labels'].to(device)\n",
    "        )\n",
    "        \n",
    "        # SOP任务\n",
    "        _, sop_losses = model(\n",
    "            input_ids=batch['sop_input_ids'].to(device),\n",
    "            attention_mask=batch['sop_attention_mask'].to(device),\n",
    "            sop_labels=batch['sop_labels'].to(device)\n",
    "        )\n",
    "        \n",
    "        # 加权总损失\n",
    "        total_loss = (\n",
    "            loss_weights['cls'] * cls_loss +\n",
    "            loss_weights['mlm'] * mlm_losses['mlm'] +\n",
    "            loss_weights['sop'] * sop_losses['sop']\n",
    "        )\n",
    "        \n",
    "        total_loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "    return total_loss.item()\n",
    "\n",
    "# 使用示例\n",
    "if __name__ == \"__main__\":\n",
    "    # 配置参数\n",
    "    BERT_MODEL = \"bert-base-chinese\"\n",
    "    NUM_CLASSES = 14\n",
    "    BATCH_SIZE = 16\n",
    "    MAX_LEN = 128\n",
    "    \n",
    "    # 初始化组件\n",
    "    tokenizer = BertTokenizer.from_pretrained(BERT_MODEL)\n",
    "    model = JointModel(BERT_MODEL, NUM_CLASSES)\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.to(device)\n",
    "    \n",
    "    # 示例数据（需替换为真实数据）\n",
    "    train_texts = [\"这是一个正样本。包含领域相关术语。\", \"这是负样本。数据增强很重要。\"]\n",
    "    train_labels = [1, 0]\n",
    "    \n",
    "    dataset = JointDataset(train_texts, train_labels, tokenizer, MAX_LEN)\n",
    "    dataloader = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "    \n",
    "    # 优化器\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=2e-5)\n",
    "    \n",
    "    # 训练1个epoch\n",
    "    loss = train_epoch(model, dataloader, optimizer, device)\n",
    "    print(f\"Training loss: {loss:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
