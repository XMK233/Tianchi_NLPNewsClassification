{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8d8114b5-ffbc-4b0a-9e85-4c007576f1c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "storage dir: /Users/minkexiu/Downloads/GitHub/Tianchi_NLPNewsClassification\n",
      "code dir: /Users/minkexiu/Documents/GitHub/Tianchi_NLPNewsClassification \n",
      "\n",
      "19 00 09\n",
      "先天八卦数: 1乾, 2兑, 3离, 4震, 5巽, 6坎, 7艮, 8坤\n",
      "本卦上：3 本卦下：8 变爻：3\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>火地晋</th>\n",
       "      <th>水山蹇</th>\n",
       "      <th>火山旅</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>上卦</th>\n",
       "      <td>☲离火</td>\n",
       "      <td>☵坎水</td>\n",
       "      <td>☲离火</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>下卦</th>\n",
       "      <td>☷坤土</td>\n",
       "      <td>☶艮土</td>\n",
       "      <td>☶艮土</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    火地晋  水山蹇  火山旅\n",
       "上卦  ☲离火  ☵坎水  ☲离火\n",
       "下卦  ☷坤土  ☶艮土  ☶艮土"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "01 22 1 子时\n",
      "先天八卦数: 1乾, 2兑, 3离, 4震, 5巽, 6坎, 7艮, 8坤\n",
      "本卦上：1 本卦下：6 变爻：1\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>天水松</th>\n",
       "      <th>风火家人</th>\n",
       "      <th>天泽履</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>上卦</th>\n",
       "      <td>☰乾金</td>\n",
       "      <td>☴巽木</td>\n",
       "      <td>☰乾金</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>下卦</th>\n",
       "      <td>☵坎水</td>\n",
       "      <td>☲离火</td>\n",
       "      <td>☱兑金</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    天水松 风火家人  天泽履\n",
       "上卦  ☰乾金  ☴巽木  ☰乾金\n",
       "下卦  ☵坎水  ☲离火  ☱兑金"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import sys\n",
    "# from kaitoupao_wsl import *\n",
    "from kaitoupao import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4d85fbf8-6949-4d26-a614-27ae9f1ecab3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, TensorDataset, Dataset\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from sklearn.metrics import f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6605d765-b333-41cc-9da7-928a2eb80d61",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "319b2f47-d25d-4943-b794-5865c4a8ff25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# device = torch.device(\"cuda\")\n",
    "device = torch.device(\"mps\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "81e0e79e-4733-4950-8e77-0598ad139549",
   "metadata": {},
   "outputs": [],
   "source": [
    "scheme_type = \"20250217_2\" "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "151910d0-6c8d-423f-87bc-538f48073940",
   "metadata": {},
   "source": [
    "# 加载训练集和测试集，将全量字符列表给它弄出来"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "80519151-9c80-4bbe-8967-ca9b9d9952af",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train = pd.read_csv(create_originalData_path(\"train_set.csv\"), sep=\"\\t\", nrows=1000)#.sample(1000) , nrows=1000\n",
    "data_test = pd.read_csv(create_originalData_path(\"test_a.csv\"), sep=\"\\t\", nrows=1000)#.sample(1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7003ef74-4ad9-435f-9234-516e9f608f2a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1000, 2), (1000, 1))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_train.shape, data_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d37fff12-c89e-4e84-9a9d-89064b711b94",
   "metadata": {},
   "outputs": [],
   "source": [
    "type_of_class = data_train.label.nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "808d37eb-c13a-4d59-923d-cd5aa4a6740b",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, valid_data = train_test_split(data_train, test_size=0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ec7669d9-fefa-4f33-8c3a-9e2764a98f39",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = train_data.reset_index(drop=True)\n",
    "valid_data = valid_data.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1bd3acc9-b6fe-4b86-a846-64ba4cd2e5ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_labels = torch.tensor(train_data.label.to_list(), dtype=torch.long)\n",
    "valid_labels = torch.tensor(valid_data.label.to_list(), dtype=torch.long)\n",
    "test_labels = torch.tensor([-1 for x in range(data_test.shape[0])], dtype=torch.long) ## test_labels的label是假的。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b1ce88e-07f2-4126-88ac-9666bfb59b0f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cb7295b-d75e-4a7d-ba04-9ec132f0ba36",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "088c49d1-c176-48fc-a2b1-be0034b79ff2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d95f4d2-2a1e-4228-91f6-e48869d35baf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5784b333-7a55-4d5d-9a71-8d2704965e70",
   "metadata": {},
   "source": [
    "## 使用TF-IDF提取特征"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "773f5b4b-bb56-4aed-aea4-19db9e9391c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "tfidf_train_features = tfidf_vectorizer.fit_transform(train_data['text'])\n",
    "tfidf_valid_features = tfidf_vectorizer.transform(valid_data['text'])\n",
    "tfidf_test_features = tfidf_vectorizer.transform(data_test['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "318aa667-e44a-46d6-9f60-af2781806313",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_features = torch.tensor(tfidf_train_features.toarray(), dtype=torch.float32)\n",
    "valid_features = torch.tensor(tfidf_valid_features.toarray(), dtype=torch.float32)\n",
    "test_features = torch.tensor(tfidf_test_features.toarray(), dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "95376078-9014-4cef-8dd1-f3a142644858",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3703"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc_input_dim = train_features.shape[1]\n",
    "sc_input_dim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6873e519-4b58-4ed5-90d6-b56bb0419929",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0c7691fe-777d-422a-8e84-015c3654d88e",
   "metadata": {},
   "source": [
    "## 准备数据集，tokenizer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "359de0fd-bc05-47e1-a624-a6d23e86b4b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 下载数据并进行预处理\n",
    "vocab_size = 8000  # 只考虑前 20k 词汇\n",
    "maxlen = 800  # 只考虑每条评论的前 200 个词"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "18d84ba3-5a7b-4384-a438-03ba49584a2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished writing file: ./vocab.txt\n"
     ]
    }
   ],
   "source": [
    "save_feaList_to_file([str(i) for i in range(vocab_size)], \"./vocab.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0f94fc5d-e2c7-41b0-a686-1fa8fe535792",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertTokenizer(name_or_path='', vocab_size=8000, model_max_length=800, is_fast=False, padding_side='right', truncation_side='right', special_tokens={'unk_token': '[UNK]', 'sep_token': '[SEP]', 'pad_token': '[PAD]', 'cls_token': '[CLS]', 'mask_token': '[MASK]'}, clean_up_tokenization_spaces=True),  added_tokens_decoder={\n",
       "\t8000: AddedToken(\"[UNK]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t8001: AddedToken(\"[SEP]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t8002: AddedToken(\"[PAD]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t8003: AddedToken(\"[CLS]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t8004: AddedToken(\"[MASK]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import BertTokenizer\n",
    "tknz = BertTokenizer(\n",
    "    vocab_file=\"vocab.txt\",\n",
    "    model_max_length=maxlen\n",
    ")\n",
    "tknz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6b898697-cfcc-4f89-b334-6abd66ec6270",
   "metadata": {},
   "outputs": [],
   "source": [
    "batchsize = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "91d67ebd-ba91-494f-b148-d3945a95b474",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyData(Dataset):\n",
    "    def __init__(\n",
    "        self, \n",
    "        ori_data, tfidf_feats, label,\n",
    "    ):\n",
    "        self.ori_data = ori_data\n",
    "        self.tfidf_feats = tfidf_feats\n",
    "        self.label = label\n",
    "        self.mlm_prob = 0.15\n",
    "        self.tokenizer = tknz\n",
    "        self.max_len = maxlen\n",
    " \n",
    "    def __len__(self):\n",
    "        return len(self.ori_data)\n",
    " \n",
    "    def __getitem__(self, idx):\n",
    "        # tuple_ = (\n",
    "        #     self.ori_data[idx], \n",
    "        #     self.tfidf_feats[idx], \n",
    "        #     self.label[idx]\n",
    "        # )\n",
    "        # return tuple_\n",
    "\n",
    "        text = self.ori_data[idx]\n",
    "        label = self.label[idx]\n",
    "        \n",
    "        # 1. 原始文本编码（用于分类）\n",
    "        encoding = self.tokenizer(\n",
    "            text,\n",
    "            max_length=self.max_len,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "\n",
    "        # 2. 生成MLM数据\n",
    "        input_ids = encoding[\"input_ids\"].squeeze(0)\n",
    "        mlm_input_ids = input_ids.clone()\n",
    "        mlm_labels = torch.full_like(input_ids, -100)  # 默认忽略非mask位置\n",
    "        \n",
    "        # 随机选择15%的token进行mask\n",
    "        rand = torch.rand(input_ids.shape)\n",
    "        mask_indices = (rand < self.mlm_prob) & (input_ids != self.tokenizer.cls_token_id) & (input_ids != self.tokenizer.sep_token_id)\n",
    "        \n",
    "        # 80%替换为[MASK], 10%随机词, 10%保持原词\n",
    "        replace_mask = mask_indices & (torch.rand(mask_indices.shape) < 0.8)\n",
    "        random_mask = mask_indices & (torch.rand(mask_indices.shape) < 0.5) & ~replace_mask\n",
    "        \n",
    "        mlm_input_ids[replace_mask] = self.tokenizer.mask_token_id\n",
    "        mlm_input_ids[random_mask] = torch.randint(0, self.tokenizer.vocab_size, (sum(random_mask),))\n",
    "        mlm_labels[mask_indices] = input_ids[mask_indices]\n",
    "\n",
    "        # 3. 生成SOP数据（句子顺序预测）\n",
    "        sentences = text.split('.')  # 简单按句号分割\n",
    "        if len(sentences) >= 2:\n",
    "            # 50%概率交换前两句\n",
    "            if torch.rand(1) < 0.5:\n",
    "                sent1, sent2 = sentences[0], sentences[1]\n",
    "                sop_label = 1  # 顺序正确\n",
    "            else:\n",
    "                sent1, sent2 = sentences[1], sentences[0]\n",
    "                sop_label = 0  # 顺序错误\n",
    "            \n",
    "            sop_text = f\"{sent1} [SEP] {sent2}\"\n",
    "            sop_encoding = self.tokenizer(\n",
    "                sop_text,\n",
    "                max_length=self.max_len,\n",
    "                padding='max_length',\n",
    "                truncation=True,\n",
    "                return_tensors='pt'\n",
    "            )\n",
    "            sop_input_ids = sop_encoding[\"input_ids\"].squeeze(0)\n",
    "            sop_attention_mask = sop_encoding[\"attention_mask\"].squeeze(0)\n",
    "        else:\n",
    "            sop_input_ids = input_ids\n",
    "            sop_attention_mask = encoding[\"attention_mask\"].squeeze(0)\n",
    "            sop_label = -100  # 忽略此样本\n",
    "\n",
    "        return (\n",
    "            {\n",
    "                \"input_ids\": input_ids,\n",
    "                \"attention_mask\": encoding[\"attention_mask\"].squeeze(0),\n",
    "                \"labels\": torch.tensor(label, dtype=torch.long),\n",
    "                \"mlm_input_ids\": mlm_input_ids,\n",
    "                \"mlm_labels\": mlm_labels,\n",
    "                \"sop_input_ids\": sop_input_ids,\n",
    "                \"sop_attention_mask\": sop_attention_mask,\n",
    "                \"sop_labels\": torch.tensor(sop_label, dtype=torch.long)\n",
    "            }, \n",
    "            self.tfidf_feats[idx],\n",
    "            label,\n",
    "       )\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c223828b-08a6-42ad-8cba-0d22c73dedcd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "bec272ff-950c-4517-8702-40349394029c",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(MyData(train_data.text, train_features, train_labels), batch_size=batchsize, shuffle=True,)\n",
    "val_loader = DataLoader(MyData(valid_data.text, valid_features, valid_labels), batch_size=batchsize, shuffle=True,)\n",
    "test_loader = DataLoader(MyData(data_test.text, test_features, test_labels), batch_size=batchsize, shuffle=False,) ## 这个不要shuffle，否则传到oj上面去就GG了。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce902369-4e1e-4b78-9d8d-597b351dc715",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "raw",
   "id": "026912bb-30d2-4c5c-89e3-09a9fb6fa8bc",
   "metadata": {},
   "source": [
    "for x in train_loader:\n",
    "    break"
   ]
  },
  {
   "cell_type": "raw",
   "id": "e82bb006-ef03-46b7-adb5-82ec6fb0c8f8",
   "metadata": {},
   "source": [
    "x[\"input_ids\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42313977-935c-499d-815d-2886154bd4a1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5ae440d3-e3d2-4907-ac77-0d97962c576a",
   "metadata": {},
   "source": [
    "# 定义网络结构"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "76464333-42d6-471a-a2db-03b311bb154f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义联合训练模型\n",
    "class JointModel(nn.Module):\n",
    "    def __init__(self, bert_model_name, num_labels, mlm_vocab_size=vocab_size):\n",
    "        super().__init__()\n",
    "        # 共享的BERT编码器\n",
    "        self.bert = BertModel(BertConfig(\n",
    "                vocab_size=vocab_size,\n",
    "                hidden_size=32,\n",
    "                num_hidden_layers=4,\n",
    "                num_attention_heads=2,\n",
    "                intermediate_size=64,\n",
    "                max_position_embeddings=maxlen,\n",
    "                num_labels=num_labels\n",
    "            ))\n",
    "        self.config = self.bert.config\n",
    "        \n",
    "        # 分类任务头\n",
    "        self.classifier = nn.Linear(self.config.hidden_size, num_labels)\n",
    "        \n",
    "        # MLM任务头\n",
    "        self.mlm_head = nn.Linear(self.config.hidden_size, mlm_vocab_size)\n",
    "        \n",
    "        # SOP任务头\n",
    "        self.sop_head = nn.Linear(self.config.hidden_size, 2)  # 二分类：顺序是否正确\n",
    "\n",
    "    def forward(self, input_ids, attention_mask, token_type_ids=None, \n",
    "                mlm_labels=None, sop_labels=None):\n",
    "        # 共享编码器输出\n",
    "        outputs = self.bert(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            token_type_ids=token_type_ids,\n",
    "            return_dict=True\n",
    "        )\n",
    "        # print(outputs)\n",
    "        sequence_output = outputs.last_hidden_state  # [batch, seq_len, hidden]\n",
    "        pooled_output = outputs.pooler_output        # [batch, hidden]\n",
    "\n",
    "        # 分类任务\n",
    "        cls_logits = self.classifier(pooled_output)  # [batch, num_labels]\n",
    "\n",
    "        # MLM任务\n",
    "        mlm_logits = self.mlm_head(sequence_output)  # [batch, seq_len, vocab]\n",
    "\n",
    "        # SOP任务\n",
    "        sop_logits = self.sop_head(pooled_output)    # [batch, 2]\n",
    "\n",
    "        # 计算各任务损失\n",
    "        losses = {}\n",
    "        if mlm_labels is not None:\n",
    "            loss_fct = nn.CrossEntropyLoss(ignore_index=-100)\n",
    "            mlm_loss = loss_fct(\n",
    "                mlm_logits.view(-1, self.config.vocab_size),\n",
    "                mlm_labels.view(-1)\n",
    "            )\n",
    "            losses[\"mlm\"] = mlm_loss\n",
    "\n",
    "        if sop_labels is not None:\n",
    "            loss_fct = nn.CrossEntropyLoss()\n",
    "            sop_loss = loss_fct(sop_logits.view(-1, 2), sop_labels.view(-1))\n",
    "            losses[\"sop\"] = sop_loss\n",
    "\n",
    "        return cls_logits, losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "535f8358-1cc9-4bd9-b93d-04975823d431",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f235d8db-587a-4b35-aad6-8286995d73b2",
   "metadata": {},
   "source": [
    "# 构建模型以及训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e3054d47-08c5-4f88-8d9b-e42155510ced",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertConfig\n",
    "from transformers import BertForSequenceClassification, BertModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ef3b3ae4-c7db-48aa-9b53-803138555eee",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = JointModel(\"\", type_of_class)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=0.001)\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f873c185-c9bc-4c08-9f4d-257022f104a6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "JointModel(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(8000, 32, padding_idx=0)\n",
       "      (position_embeddings): Embedding(800, 32)\n",
       "      (token_type_embeddings): Embedding(2, 32)\n",
       "      (LayerNorm): LayerNorm((32,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-3): 4 x BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSdpaSelfAttention(\n",
       "              (query): Linear(in_features=32, out_features=32, bias=True)\n",
       "              (key): Linear(in_features=32, out_features=32, bias=True)\n",
       "              (value): Linear(in_features=32, out_features=32, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=32, out_features=32, bias=True)\n",
       "              (LayerNorm): LayerNorm((32,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=32, out_features=64, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=64, out_features=32, bias=True)\n",
       "            (LayerNorm): LayerNorm((32,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=32, out_features=32, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (classifier): Linear(in_features=32, out_features=14, bias=True)\n",
       "  (mlm_head): Linear(in_features=32, out_features=8000, bias=True)\n",
       "  (sop_head): Linear(in_features=32, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "raw",
   "id": "0eb6961a-95c8-4eaf-82ff-34e231ac145f",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "model.to(device)\n",
    "\n",
    "for epoch in range(1):\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for batch, input_tfidfs, targets  in tqdm.tqdm(train_loader):\n",
    "        input_tfidfs, targets = input_tfidfs.to(device), targets.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        # 分类任务\n",
    "        cls_output, _ = model(\n",
    "            input_ids=batch['input_ids'].to(device),\n",
    "            attention_mask=batch['attention_mask'].to(device)\n",
    "        )\n",
    "        cls_loss = nn.CrossEntropyLoss()(cls_output, batch['labels'].to(device))\n",
    "        # MLM任务\n",
    "        _, mlm_losses = model(\n",
    "            input_ids=batch['mlm_input_ids'].to(device),\n",
    "            attention_mask=batch['attention_mask'].to(device),\n",
    "            mlm_labels=batch['mlm_labels'].to(device)\n",
    "        )\n",
    "        # SOP任务\n",
    "        _, sop_losses = model(\n",
    "            input_ids=batch['sop_input_ids'].to(device),\n",
    "            attention_mask=batch['sop_attention_mask'].to(device),\n",
    "            sop_labels=batch['sop_labels'].to(device)\n",
    "        )\n",
    "        # 加权总损失\n",
    "        total_loss = (\n",
    "            loss_weights['cls'] * cls_loss +\n",
    "            loss_weights['mlm'] * mlm_losses['mlm'] +\n",
    "            loss_weights['sop'] * sop_losses['sop']\n",
    "        )\n",
    "        total_loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        \n",
    "        train_loss += total_loss.item()\n",
    "        _, predicted = torch.max(cls_output, 1)\n",
    "        total += targets.size(0)\n",
    "        correct += predicted.eq(targets).sum().item()\n",
    "    \n",
    "    model.eval()\n",
    "    val_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        total_predicted = []\n",
    "        total_label = []\n",
    "        for batch, input_tfidfs, targets in tqdm.tqdm(val_loader):\n",
    "            input_tfidfs, targets = input_tfidfs.to(device), targets.to(device)\n",
    "            outputs, _ = model(\n",
    "                input_ids=batch['input_ids'].to(device),\n",
    "                attention_mask=batch['attention_mask'].to(device)\n",
    "            )\n",
    "            loss = criterion(outputs, targets)\n",
    "            val_loss += loss.item()\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            total += targets.size(0)\n",
    "            correct += predicted.eq(targets).sum().item()\n",
    "\n",
    "            total_predicted += list(predicted.cpu())\n",
    "            total_label += list(targets.cpu())\n",
    "    f1 = f1_score(total_label, total_predicted, average='macro')\n",
    "            \n",
    "    print(f'Validation Loss: {val_loss/len(val_loader)}, Accuracy: {100.*correct/total}%, f1 score is {f1}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "e017e6d7-d8d8-4eee-99a2-ac17a9716c3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import shutil\n",
    "\n",
    "# 训练和评估模型\n",
    "def train_model(model, train_loader, val_loader, criterion, optimizer, epochs=2):\n",
    "\n",
    "    ## loss_weights仅用于无监督预训练场景：\n",
    "    loss_weights={'cls': 1.0, 'mlm': 0.5, 'sop': 0.5}\n",
    "    \n",
    "    model.to(device)\n",
    "    \n",
    "    log_path = \"runs/Logs\"\n",
    "    if os.path.exists(\n",
    "        os.path.dirname(log_path)\n",
    "    ):\n",
    "        shutil.rmtree(os.path.dirname(log_path))\n",
    "        \n",
    "    writer = SummaryWriter(log_dir=log_path)\n",
    "    \n",
    "    total_train_step = 0\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        loss_sum = 0\n",
    "        for batch, input_tfidfs, targets in tqdm.tqdm(train_loader):\n",
    "            input_tfidfs, targets = input_tfidfs.to(device), targets.to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            # 分类任务\n",
    "            cls_output, _ = model(\n",
    "                input_ids=batch['input_ids'].to(device),\n",
    "                attention_mask=batch['attention_mask'].to(device)\n",
    "            )\n",
    "            cls_loss = nn.CrossEntropyLoss()(cls_output, batch['labels'].to(device))\n",
    "            # MLM任务\n",
    "            _, mlm_losses = model(\n",
    "                input_ids=batch['mlm_input_ids'].to(device),\n",
    "                attention_mask=batch['attention_mask'].to(device),\n",
    "                mlm_labels=batch['mlm_labels'].to(device)\n",
    "            )\n",
    "            # SOP任务\n",
    "            _, sop_losses = model(\n",
    "                input_ids=batch['sop_input_ids'].to(device),\n",
    "                attention_mask=batch['sop_attention_mask'].to(device),\n",
    "                sop_labels=batch['sop_labels'].to(device)\n",
    "            )\n",
    "            # 加权总损失\n",
    "            total_loss = (\n",
    "                loss_weights['cls'] * cls_loss +\n",
    "                loss_weights['mlm'] * mlm_losses['mlm'] +\n",
    "                loss_weights['sop'] * sop_losses['sop']\n",
    "            )\n",
    "            total_loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            train_loss += total_loss.item()\n",
    "            _, predicted = torch.max(cls_output, 1)\n",
    "            total += targets.size(0)\n",
    "            correct += predicted.eq(targets).sum().item()\n",
    "            \n",
    "            total_train_step += 1\n",
    "            loss_sum += total_loss.item()\n",
    "            if total_train_step % 10 == 0:\n",
    "                # print(total_train_step//10)\n",
    "                writer.add_scalar(f\"train_loss_detail-{scheme_type}\", loss_sum/10, total_train_step//10)\n",
    "                loss_sum = 0\n",
    "                \n",
    "        print(f'Epoch {epoch + 1}/{epochs}, Loss: {train_loss/len(train_loader)}, Accuracy: {100.*correct/total}%')\n",
    "        writer.add_scalar(f\"epoch_loss-{scheme_type}\", train_loss/len(train_loader), epoch+1)\n",
    "        \n",
    "        model.eval()\n",
    "        val_loss = 0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        with torch.no_grad():\n",
    "            total_predicted = []\n",
    "            total_label = []\n",
    "            for batch, input_tfidfs, targets in tqdm.tqdm(val_loader):\n",
    "                input_tfidfs, targets = input_tfidfs.to(device), targets.to(device)\n",
    "                outputs, _ = model(\n",
    "                    input_ids=batch['input_ids'].to(device),\n",
    "                    attention_mask=batch['attention_mask'].to(device)\n",
    "                )\n",
    "                loss = criterion(outputs, targets)\n",
    "                val_loss += loss.item()\n",
    "                _, predicted = torch.max(outputs, 1)\n",
    "                total += targets.size(0)\n",
    "                correct += predicted.eq(targets).sum().item()\n",
    "\n",
    "                total_predicted += list(predicted.cpu())\n",
    "                total_label += list(targets.cpu())\n",
    "        f1 = f1_score(total_label, total_predicted, average='macro')\n",
    "                \n",
    "        print(f'Validation Loss: {val_loss/len(val_loader)}, Accuracy: {100.*correct/total}%, f1 score is {f1}')\n",
    "    writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "38b789c8-0cf8-4e0d-897d-b42c874d11ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                    | 0/22 [00:00<?, ?it/s]/var/folders/s1/1jpfx0m52rj4k7cgqkh7g3q40000gn/T/ipykernel_55074/3999588957.py:83: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  \"labels\": torch.tensor(label, dtype=torch.long),\n",
      " 45%|███████████████████▌                       | 10/22 [00:08<00:09,  1.22it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 91%|███████████████████████████████████████    | 20/22 [00:16<00:01,  1.21it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████| 22/22 [00:18<00:00,  1.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1, Loss: 5.608910647305575, Accuracy: 14.714285714285714%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████| 10/10 [00:02<00:00,  4.26it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 2.3554890394210815, Accuracy: 19.333333333333332%, f1 score is 0.02314445331205108\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "train_model(model, train_loader, val_loader, criterion, optimizer, epochs=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9093fdc5-3313-4dc5-9c73-410efd562c15",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "e60fa936-d26b-45e5-8080-5801fd81c36b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_model(model, train_loader, val_loader, criterion, optimizer, epochs=4) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "9b0d5350-dc83-41f7-a4a1-33b72dcae369",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_model(model, train_loader, val_loader, criterion, optimizer, epochs=1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "cefe3c44-ea12-4b15-ac3f-a6eaf1e334c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_model(model, train_loader, val_loader, criterion, optimizer, epochs=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "19e6ef7b-2814-44f5-b825-49f11c901b48",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_model(model, train_loader, val_loader, criterion, optimizer, epochs=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5564b20-6245-4169-a865-77f7e2ba798b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "d282441a-061d-49bd-bcdf-9307e9bba6c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# run_finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d981286a-b45e-444d-956d-40015c302121",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "fda2273a-b9be-48bb-b6a7-bbba54681dca",
   "metadata": {},
   "source": [
    "# 保存模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "c84a5ca3-21e1-466c-aeff-bbf6a4b94868",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/minkexiu/Downloads/GitHub/Tianchi_NLPNewsClassification/trained_models/ori_9319-20250217_2.pkl\n"
     ]
    }
   ],
   "source": [
    "save_pickle_object(model, create_trained_models_path(f\"ori_9319-{scheme_type}.pkl\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "453e182b-c62a-44fd-afc5-97c4a1495f08",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e28f73fe-c05f-45fb-ac86-437d8adad664",
   "metadata": {},
   "source": [
    "# 预测一下试试"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "f419efd0-8862-48ce-814f-1fae5bd1641b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = load_pickle_object(create_trained_models_path(f\"ori_9319-{scheme_type}.pkl\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62890fb3-759f-4b58-8686-0ca62ba903d7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "0a123afe-2fe4-4f8a-903b-1b24f4810a7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                    | 0/32 [00:00<?, ?it/s]/var/folders/s1/1jpfx0m52rj4k7cgqkh7g3q40000gn/T/ipykernel_55074/3999588957.py:83: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  \"labels\": torch.tensor(label, dtype=torch.long),\n",
      "100%|███████████████████████████████████████████| 32/32 [00:05<00:00,  5.74it/s]\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    total_predicted = []\n",
    "    for batch, input_tfidfs, targets in tqdm.tqdm(test_loader):\n",
    "        input_tfidfs, targets = input_tfidfs.to(device), targets.to(device)\n",
    "        outputs, _ = model(\n",
    "            input_ids=batch['input_ids'].to(device),\n",
    "            attention_mask=batch['attention_mask'].to(device)\n",
    "        )\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        total_predicted += list(predicted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "706d090f-a3bc-45f4-81f2-e85fef155296",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1000"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "oot_rst = [int(x) for x in total_predicted]\n",
    "len(oot_rst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "b7bd7c9e-be76-4c3e-89c7-beae8a4ccf90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df.to_csv(\"/Users/minkexiu/Downloads/GitHub/Tianchi_NLPNewsClassification/preprocessedData/rst-20250217_2.csv\", index=False)\n",
      "data saved.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'/Users/minkexiu/Downloads/GitHub/Tianchi_NLPNewsClassification/preprocessedData/rst-20250217_2.csv'"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "store_data_to_newbasepath(pd.DataFrame({\"label\": oot_rst}), f\"rst-{scheme_type}\", fmt=\"csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3db65fc-dd7b-4ca6-a420-bbe43e95b9b6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
