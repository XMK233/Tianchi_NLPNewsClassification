{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bafa37f5-3428-477e-ac37-68ce3021b3d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random, os, tqdm, time, json, re, IPython, zhdate, sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from matplotlib.font_manager import FontProperties\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime\n",
    "\n",
    "sys.path.append(\"../../../\")\n",
    "\n",
    "random.seed(618)\n",
    "np.random.seed(907)\n",
    "\n",
    "tqdm.tqdm.pandas() ## 引入这个，就可以在apply的时候用progress_apply了。\n",
    "\n",
    "# sys.path.append(\"../../../\")\n",
    "new_base_path = os.path.join(\n",
    "    \"/mnt\",\n",
    "    \"/\".join(\n",
    "        os.getcwd().split(\"/\")[-1*(len(sys.path[-1].split(\"/\")) - 1):]\n",
    "    ),\n",
    ")\n",
    "print(\"storage dir:\", new_base_path)\n",
    "print(\"code dir:\", os.getcwd(), \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16e41145-579c-4909-adbe-69b88223c26f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "251a847b-d8e7-4973-9e19-3ca4cad0f3c7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49c5c772-5fc7-433d-b621-5f5809d5fb77",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8d8114b5-ffbc-4b0a-9e85-4c007576f1c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "storage dir: /mnt/d/forCoding_code/Tianchi_NLPNewsClassification\n",
      "code dir: /mnt/d/forCoding_code/Tianchi_NLPNewsClassification \n",
      "\n",
      "19 18 08\n",
      "先天八卦数: 1乾, 2兑, 3离, 4震, 5巽, 6坎, 7艮, 8坤\n",
      "本卦上：3 本卦下：2 变爻：2\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>火泽睽</th>\n",
       "      <th>水火既济</th>\n",
       "      <th>火雷噬嗑</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>上卦</th>\n",
       "      <td>☲离火</td>\n",
       "      <td>☵坎水</td>\n",
       "      <td>☲离火</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>下卦</th>\n",
       "      <td>☱兑金</td>\n",
       "      <td>☲离火</td>\n",
       "      <td>☳震木</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    火泽睽 水火既济 火雷噬嗑\n",
       "上卦  ☲离火  ☵坎水  ☲离火\n",
       "下卦  ☱兑金  ☲离火  ☳震木"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12 20 10 酉时\n",
      "先天八卦数: 1乾, 2兑, 3离, 4震, 5巽, 6坎, 7艮, 8坤\n",
      "本卦上：4 本卦下：4 变爻：4\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>雷雷震</th>\n",
       "      <th>水山蹇</th>\n",
       "      <th>地雷复</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>上卦</th>\n",
       "      <td>☳震木</td>\n",
       "      <td>☵坎水</td>\n",
       "      <td>☷坤土</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>下卦</th>\n",
       "      <td>☳震木</td>\n",
       "      <td>☶艮土</td>\n",
       "      <td>☳震木</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    雷雷震  水山蹇  地雷复\n",
       "上卦  ☳震木  ☵坎水  ☷坤土\n",
       "下卦  ☳震木  ☶艮土  ☳震木"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'joblib'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01msys\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkaitoupao_wsl\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n",
      "File \u001b[0;32m/mnt/d/forCoding_code/Tianchi_NLPNewsClassification/kaitoupao_wsl.py:292\u001b[0m\n\u001b[1;32m    289\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m three_num_get_gua(\u001b[38;5;28mint\u001b[39m(n1), \u001b[38;5;28mint\u001b[39m(n2), \u001b[38;5;28mint\u001b[39m(n3))\n\u001b[1;32m    290\u001b[0m easy_start_gua_lunar()\n\u001b[0;32m--> 292\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpickle\u001b[39;00m\u001b[38;5;241m,\u001b[39m \u001b[38;5;21;01mjoblib\u001b[39;00m\n\u001b[1;32m    293\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msave_pickle_object\u001b[39m(obj, path):\n\u001b[1;32m    294\u001b[0m     model_path \u001b[38;5;241m=\u001b[39m path\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'joblib'"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "from kaitoupao_wsl import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3c8a930-8ebd-4192-98c7-1b320015e269",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d255989-e60b-40a5-b3e5-9e6365417a3c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cb7db14-e021-4200-b2b9-2a1dd0834020",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fb373e1-4b0a-4f08-b818-1a35f7baddf2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "895ce492-8832-4eab-8756-c87e86baf92a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d85fbf8-6949-4d26-a614-27ae9f1ecab3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, TensorDataset, Dataset\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from sklearn.metrics import f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6605d765-b333-41cc-9da7-928a2eb80d61",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "319b2f47-d25d-4943-b794-5865c4a8ff25",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "151910d0-6c8d-423f-87bc-538f48073940",
   "metadata": {},
   "source": [
    "# 加载训练集和测试集，将全量字符列表给它弄出来"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "80519151-9c80-4bbe-8967-ca9b9d9952af",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train = pd.read_csv(create_originalData_path(\"train_set.csv\"), sep=\"\\t\")#.sample(1000)\n",
    "data_test = pd.read_csv(create_originalData_path(\"test_a.csv\"), sep=\"\\t\")#.sample(1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7003ef74-4ad9-435f-9234-516e9f608f2a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((200000, 2), (50000, 1))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_train.shape, data_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7a541cca-8a5b-44e5-8405-acb47a774851",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train = data_train.reset_index().rename(columns={\"index\": \"data_id\"})\n",
    "data_test = data_test.reset_index().rename(columns={\"index\": \"data_id\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b4c005d-05b0-4263-9f2e-261d143dfe5a",
   "metadata": {},
   "source": [
    "## 拆分开"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d37fff12-c89e-4e84-9a9d-89064b711b94",
   "metadata": {},
   "outputs": [],
   "source": [
    "type_of_class = data_train.label.nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "808d37eb-c13a-4d59-923d-cd5aa4a6740b",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, valid_data = train_test_split(data_train, test_size=0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "888c045e-db32-42e3-a060-ee26a914b08b",
   "metadata": {},
   "outputs": [],
   "source": [
    "maxlen = 800  # 只考虑每条评论的前 200 个词\n",
    "def split_list(lst, max_length):\n",
    "    return [lst[i:i + max_length] for i in range(0, len(lst), max_length)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a49bd0dd-9856-4017-bd7f-e09d270bed2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "140000it [00:05, 27919.45it/s]\n",
      "60000it [00:02, 28380.01it/s]\n",
      "50000it [00:01, 29087.90it/s]\n"
     ]
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "#######################################\n",
    "new_rst = defaultdict(list)\n",
    "for idx, row in tqdm.tqdm(train_data.iterrows()):\n",
    "    idn = row[\"data_id\"]\n",
    "    lbl = row[\"label\"]\n",
    "    txt = row[\"text\"].strip().split()\n",
    "    \n",
    "    for spt in split_list(txt, maxlen):\n",
    "        new_rst[\"data_id\"].append(idn)\n",
    "        new_rst[\"label\"].append(lbl)\n",
    "        new_rst[\"text\"].append(\" \".join(spt))\n",
    "train_data = pd.DataFrame(new_rst)\n",
    "#######################################\n",
    "new_rst = defaultdict(list)\n",
    "for idx, row in tqdm.tqdm(valid_data.iterrows()):\n",
    "    idn = row[\"data_id\"]\n",
    "    lbl = row[\"label\"]\n",
    "    txt = row[\"text\"].strip().split()\n",
    "    \n",
    "    for spt in split_list(txt, maxlen):\n",
    "        new_rst[\"data_id\"].append(idn)\n",
    "        new_rst[\"label\"].append(lbl)\n",
    "        new_rst[\"text\"].append(\" \".join(spt))\n",
    "valid_data = pd.DataFrame(new_rst)\n",
    "#######################################\n",
    "new_rst = defaultdict(list)\n",
    "for idx, row in tqdm.tqdm(data_test.iterrows()):\n",
    "    idn = row[\"data_id\"]\n",
    "    # lbl = row[\"label\"]\n",
    "    txt = row[\"text\"].strip().split()\n",
    "    \n",
    "    for spt in split_list(txt, maxlen):\n",
    "        new_rst[\"data_id\"].append(idn)\n",
    "        # new_rst[\"label\"].append(lbl)\n",
    "        new_rst[\"text\"].append(\" \".join(spt))\n",
    "data_test = pd.DataFrame(new_rst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1ced50a7-bd84-48c9-863d-a7d2ff891436",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((231860, 3), (99757, 3), (83152, 2))"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.shape, valid_data.shape, data_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa310b08-8a3a-4831-86ae-cdf86dc9df2d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1bd3acc9-b6fe-4b86-a846-64ba4cd2e5ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_labels = torch.tensor(train_data.label.to_list(), dtype=torch.long)\n",
    "valid_labels = torch.tensor(valid_data.label.to_list(), dtype=torch.long)\n",
    "test_labels = torch.tensor([-1 for x in range(data_test.shape[0])], dtype=torch.long) ## test_labels的label是假的。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd8aedbd-696c-457d-894f-8acc51bcdc61",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fe9f3e69-cf9d-4a11-a3bf-0e039c88e9b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "id_train = torch.tensor(list(train_data.data_id), dtype=torch.long)\n",
    "id_valid = torch.tensor(list(valid_data.data_id), dtype=torch.long)\n",
    "id_test = torch.tensor(list(data_test.data_id), dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eed8671a-35a9-4f4f-9d93-125b7af682d0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5784b333-7a55-4d5d-9a71-8d2704965e70",
   "metadata": {},
   "source": [
    "## 使用TF-IDF提取特征"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "773f5b4b-bb56-4aed-aea4-19db9e9391c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "tfidf_train_features = tfidf_vectorizer.fit_transform(train_data['text'])\n",
    "tfidf_valid_features = tfidf_vectorizer.transform(valid_data['text'])\n",
    "tfidf_test_features = tfidf_vectorizer.transform(data_test['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "318aa667-e44a-46d6-9f60-af2781806313",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_features = torch.tensor(tfidf_train_features.toarray(), dtype=torch.float32)\n",
    "valid_features = torch.tensor(tfidf_valid_features.toarray(), dtype=torch.float32)\n",
    "test_features = torch.tensor(tfidf_test_features.toarray(), dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "95376078-9014-4cef-8dd1-f3a142644858",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6695"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc_input_dim = train_features.shape[1]\n",
    "sc_input_dim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45b8ce43-cdfc-4faf-b751-9ea99aa2e5c4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "27d9d9c0-a3f6-46a0-b88b-5a0399cc88ec",
   "metadata": {},
   "source": [
    "## 创建适合于语言序列的数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "359de0fd-bc05-47e1-a624-a6d23e86b4b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 下载数据并进行预处理\n",
    "vocab_size = 8000  # 只考虑前 20k 词汇"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "46ac102a-e2fb-478a-b2c6-6998ed02866f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_seq_str_2_int(seq, len_lim = maxlen):\n",
    "    rst = [int(wd) for idx, wd in enumerate(seq.strip().split()) if idx < len_lim]\n",
    "    return rst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f609e647-eea4-4674-8fe9-3c39f4a604c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = [torch.tensor(preprocess_seq_str_2_int(seq), dtype=torch.long) for seq in train_data.text]\n",
    "x_valid = [torch.tensor(preprocess_seq_str_2_int(seq), dtype=torch.long) for seq in valid_data.text]\n",
    "x_test = [torch.tensor(preprocess_seq_str_2_int(seq), dtype=torch.long) for seq in data_test.text]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "582fb80e-de88-4bc9-907b-d7b49b8e5753",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = pad_sequence(x_train, batch_first=True, padding_value=0)\n",
    "x_valid = pad_sequence(x_valid, batch_first=True, padding_value=0)\n",
    "x_test = pad_sequence(x_test, batch_first=True, padding_value=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6873e519-4b58-4ed5-90d6-b56bb0419929",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0c7691fe-777d-422a-8e84-015c3654d88e",
   "metadata": {},
   "source": [
    "## 准备数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "6b898697-cfcc-4f89-b334-6abd66ec6270",
   "metadata": {},
   "outputs": [],
   "source": [
    "batchsize = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "91d67ebd-ba91-494f-b148-d3945a95b474",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyData(Dataset):\n",
    "    def __init__(\n",
    "        self, \n",
    "        ori_data, tfidf_feats, label,\n",
    "        data_id,\n",
    "    ):\n",
    "        self.ori_data = ori_data\n",
    "        self.tfidf_feats = tfidf_feats\n",
    "        self.label = label\n",
    "        self.data_id = data_id\n",
    " \n",
    "    def __len__(self):\n",
    "        return len(self.ori_data)\n",
    " \n",
    "    def __getitem__(self, idx):\n",
    "        tuple_ = (\n",
    "            self.ori_data[idx], \n",
    "            self.tfidf_feats[idx], \n",
    "            self.label[idx], \n",
    "            self.data_id[idx]\n",
    "        )\n",
    "        return tuple_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "bec272ff-950c-4517-8702-40349394029c",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(MyData(x_train, train_features, train_labels, id_train), batch_size=batchsize, shuffle=True,)\n",
    "val_loader = DataLoader(MyData(x_valid, valid_features, valid_labels, id_valid), batch_size=batchsize, shuffle=True,)\n",
    "test_loader = DataLoader(MyData(x_test, test_features, test_labels, id_test), batch_size=batchsize, shuffle=True,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42313977-935c-499d-815d-2886154bd4a1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5ae440d3-e3d2-4907-ac77-0d97962c576a",
   "metadata": {},
   "source": [
    "# 定义网络结构"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a123d3a0-63ad-4991-af25-51fceabb302a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, embed_dim, num_heads, ff_dim, dropout=0.1):\n",
    "        super(TransformerBlock, self).__init__()\n",
    "        self.att = nn.MultiheadAttention(embed_dim, num_heads)\n",
    "        self.ffn = nn.Sequential(\n",
    "            nn.Linear(embed_dim, ff_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(ff_dim, embed_dim)\n",
    "        )\n",
    "        self.layernorm1 = nn.LayerNorm(embed_dim)\n",
    "        self.layernorm2 = nn.LayerNorm(embed_dim)\n",
    "        self.dropout1 = nn.Dropout(dropout)\n",
    "        self.dropout2 = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        attn_output, _ = self.att(x, x, x)\n",
    "        attn_output = self.dropout1(attn_output)\n",
    "        out1 = self.layernorm1(x + attn_output)\n",
    "        ffn_output = self.ffn(out1)\n",
    "        ffn_output = self.dropout2(ffn_output)\n",
    "        return self.layernorm2(out1 + ffn_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "e3ab05aa-f9ba-4ce2-b1d4-4d5b0725fe8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TokenAndPositionEmbedding(nn.Module):\n",
    "    def __init__(self, maxlen, vocab_size, embed_dim):\n",
    "        super(TokenAndPositionEmbedding, self).__init__()\n",
    "        self.token_emb = nn.Embedding(vocab_size, embed_dim)\n",
    "        self.pos_emb = nn.Embedding(maxlen, embed_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        maxlen = x.size(1)\n",
    "        positions = torch.arange(0, maxlen, device=x.device).unsqueeze(0).expand_as(x)\n",
    "        return self.token_emb(x) + self.pos_emb(positions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "e6a1e31c-1b93-4476-ac9f-f36d6f23b622",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 单层神经网络：\n",
    "class SentimentClassifier(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super().__init__()\n",
    "        self.fc = nn.Linear(input_dim, output_dim)\n",
    " \n",
    "    def forward(self, x):\n",
    "        return self.fc(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "7983b98b-75ee-45cc-82c3-d3414861cfb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义模型\n",
    "class TransformerModel(nn.Module):\n",
    "    def __init__(self, maxlen, vocab_size, embed_dim, num_heads, ff_dim):\n",
    "        super(TransformerModel, self).__init__()\n",
    "        self.embedding_layer = TokenAndPositionEmbedding(maxlen, vocab_size, embed_dim)\n",
    "        self.transformer_block = TransformerBlock(embed_dim, num_heads, ff_dim)\n",
    "        self.global_avg_pool = nn.AdaptiveAvgPool1d(1)\n",
    "        self.dropout1 = nn.Dropout(0.1)\n",
    "        self.dense1 = nn.Linear(embed_dim, 20)\n",
    "        self.dropout2 = nn.Dropout(0.1)\n",
    "        self.dense2 = nn.Linear(20, type_of_class)\n",
    "\n",
    "        self.sc_net = SentimentClassifier(sc_input_dim, type_of_class)\n",
    "        \n",
    "    def forward(self, x, x_tfidf):\n",
    "        x = self.embedding_layer(x).transpose(0, 1)  # Transformer expects (seq_len, batch_size, embed_dim)\n",
    "        x = self.transformer_block(x)\n",
    "        x = x.transpose(0, 1)\n",
    "        x = self.global_avg_pool(x.permute(0, 2, 1)).squeeze(-1)\n",
    "        x = self.dropout1(x)\n",
    "        x = F.relu(self.dense1(x))\n",
    "        x = self.dropout2(x)\n",
    "        return F.log_softmax(self.dense2(x) + self.sc_net(x_tfidf), dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b553712c-babe-4b12-8257-0902ded0268c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f235d8db-587a-4b35-aad6-8286995d73b2",
   "metadata": {},
   "source": [
    "# 构建模型以及训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "ef3b3ae4-c7db-48aa-9b53-803138555eee",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = TransformerModel(maxlen, vocab_size, embed_dim=64, num_heads=4, ff_dim=32)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "f873c185-c9bc-4c08-9f4d-257022f104a6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TransformerModel(\n",
       "  (embedding_layer): TokenAndPositionEmbedding(\n",
       "    (token_emb): Embedding(8000, 64)\n",
       "    (pos_emb): Embedding(800, 64)\n",
       "  )\n",
       "  (transformer_block): TransformerBlock(\n",
       "    (att): MultiheadAttention(\n",
       "      (out_proj): NonDynamicallyQuantizableLinear(in_features=64, out_features=64, bias=True)\n",
       "    )\n",
       "    (ffn): Sequential(\n",
       "      (0): Linear(in_features=64, out_features=32, bias=True)\n",
       "      (1): ReLU()\n",
       "      (2): Linear(in_features=32, out_features=64, bias=True)\n",
       "    )\n",
       "    (layernorm1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "    (layernorm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "    (dropout1): Dropout(p=0.1, inplace=False)\n",
       "    (dropout2): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (global_avg_pool): AdaptiveAvgPool1d(output_size=1)\n",
       "  (dropout1): Dropout(p=0.1, inplace=False)\n",
       "  (dense1): Linear(in_features=64, out_features=20, bias=True)\n",
       "  (dropout2): Dropout(p=0.1, inplace=False)\n",
       "  (dense2): Linear(in_features=20, out_features=14, bias=True)\n",
       "  (sc_net): SentimentClassifier(\n",
       "    (fc): Linear(in_features=6695, out_features=14, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "bc2dc0a2-2513-484b-b0f2-125f48d7cdd4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TransformerModel(\n",
       "  (embedding_layer): TokenAndPositionEmbedding(\n",
       "    (token_emb): Embedding(8000, 64)\n",
       "    (pos_emb): Embedding(800, 64)\n",
       "  )\n",
       "  (transformer_block): TransformerBlock(\n",
       "    (att): MultiheadAttention(\n",
       "      (out_proj): NonDynamicallyQuantizableLinear(in_features=64, out_features=64, bias=True)\n",
       "    )\n",
       "    (ffn): Sequential(\n",
       "      (0): Linear(in_features=64, out_features=32, bias=True)\n",
       "      (1): ReLU()\n",
       "      (2): Linear(in_features=32, out_features=64, bias=True)\n",
       "    )\n",
       "    (layernorm1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "    (layernorm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "    (dropout1): Dropout(p=0.1, inplace=False)\n",
       "    (dropout2): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (global_avg_pool): AdaptiveAvgPool1d(output_size=1)\n",
       "  (dropout1): Dropout(p=0.1, inplace=False)\n",
       "  (dense1): Linear(in_features=64, out_features=20, bias=True)\n",
       "  (dropout2): Dropout(p=0.1, inplace=False)\n",
       "  (dense2): Linear(in_features=20, out_features=14, bias=True)\n",
       "  (sc_net): SentimentClassifier(\n",
       "    (fc): Linear(in_features=6695, out_features=14, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "312f8175-f042-4d72-b289-5eaecbcce18b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_final_perf(train_list):\n",
    "    tst_df = pd.DataFrame(train_list)\n",
    "    \n",
    "    tst_df1 = tst_df.groupby(\"dids\")[\"outputs\"].apply(lambda x: x.tolist()).reset_index()\n",
    "    tst_df1[\"mean_val\"] = tst_df1.outputs.apply(lambda x: torch.tensor(x, dtype=torch.float32).mean(axis=0))\n",
    "    \n",
    "    tst_df_final = tst_df1.merge(tst_df[[\"dids\", \"targets\"]].drop_duplicates())\n",
    "    \n",
    "    final_outputs = torch.tensor(np.array(tst_df_final.mean_val.to_list()))\n",
    "    final_targets = torch.tensor(tst_df_final.targets.to_list(), dtype=torch.long)\n",
    "    \n",
    "    _, final_predicted = torch.max(final_outputs, 1)\n",
    "    print(f'accuracy: {final_predicted.eq(final_targets).sum().item()/final_predicted.shape[0]}, f1 score: {f1_score(final_targets, final_predicted, average=\"macro\")}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37d370b0-6e2c-486d-b91f-43acb0bff89d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "raw",
   "id": "773f7bd5-3cd1-49d0-9684-6ad7fc078403",
   "metadata": {},
   "source": [
    "epoch = 0\n",
    "epochs = 5\n",
    "\n",
    "\n",
    "model.train()\n",
    "train_loss = 0\n",
    "correct = 0\n",
    "total = 0\n",
    "train_list = defaultdict(list)\n",
    "\n",
    "for inputs, input_tfidfs, targets, dids in tqdm.tqdm(train_loader):\n",
    "    inputs, input_tfidfs, targets = inputs.to(device), input_tfidfs.to(device), targets.to(device)\n",
    "    optimizer.zero_grad()\n",
    "    outputs = model(inputs, input_tfidfs)\n",
    "    loss = criterion(outputs, targets)\n",
    "    loss.requires_grad_(True)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    train_loss += loss.item()\n",
    "    _, predicted = torch.max(outputs, 1)\n",
    "    total += targets.size(0)\n",
    "    correct += predicted.eq(targets).sum().item()\n",
    "\n",
    "    train_list[\"outputs\"] += outputs.tolist()\n",
    "    train_list[\"dids\"] += dids.tolist()\n",
    "    train_list[\"targets\"] += targets.tolist()\n",
    "    \n",
    "    # break\n",
    "print(f'Epoch {epoch + 1}/{epochs}, Loss: {train_loss/len(train_loader)}, Accuracy: {100.*correct/total}%')\n",
    "\n",
    "\n",
    "model.eval()\n",
    "val_loss = 0\n",
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "\n",
    "\n",
    "with torch.no_grad():\n",
    "    total_predicted = []\n",
    "    total_label = []\n",
    "    \n",
    "    valid_list = defaultdict(list)\n",
    "    \n",
    "    for inputs, input_tfidfs, targets, dids in tqdm.tqdm(val_loader):\n",
    "        inputs, input_tfidfs, targets = inputs.to(device), input_tfidfs.to(device), targets.to(device)\n",
    "        outputs = model(inputs, input_tfidfs)\n",
    "        loss = criterion(outputs, targets)\n",
    "        val_loss += loss.item()\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        total += targets.size(0)\n",
    "        correct += predicted.eq(targets).sum().item()\n",
    "\n",
    "        total_predicted += list(predicted)\n",
    "        total_label += list(targets)\n",
    "\n",
    "        valid_list[\"outputs\"] += outputs.tolist()\n",
    "        valid_list[\"dids\"] += dids.tolist()\n",
    "        valid_list[\"targets\"] += targets.tolist()\n",
    "\n",
    "        # break\n",
    "        \n",
    "f1 = f1_score(total_label, total_predicted, average='macro')\n",
    "        \n",
    "print(f'Validation Loss: {val_loss/len(val_loader)}, Accuracy: {100.*correct/total}%, f1 score is {f1}')"
   ]
  },
  {
   "cell_type": "raw",
   "id": "9383534d-bafb-4628-b064-aaf4c42b3557",
   "metadata": {},
   "source": [
    "get_final_perf(train_list)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "3e47b6da-7a26-4ad4-968b-5aac28da1e40",
   "metadata": {},
   "source": [
    "get_final_perf(valid_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1550910c-d561-4ffd-8d26-130b9e488192",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e065f39-0a9c-42fb-98ce-0663e034ae9a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "792c4f69-27eb-4e03-9f0f-3327dbad0f54",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "be7ce043-bef9-4240-9f2c-4801a077760c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 训练和评估模型\n",
    "def train_model(model, train_loader, val_loader, criterion, optimizer, epochs=2):\n",
    "    # device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    model.to(device)\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "\n",
    "        train_list = defaultdict(list)\n",
    "        \n",
    "        for inputs, input_tfidfs, targets, dids in tqdm.tqdm(train_loader):\n",
    "            inputs, input_tfidfs, targets = inputs.to(device), input_tfidfs.to(device), targets.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs, input_tfidfs)\n",
    "            loss = criterion(outputs, targets)\n",
    "            loss.requires_grad_(True)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_loss += loss.item()\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            total += targets.size(0)\n",
    "            correct += predicted.eq(targets).sum().item()\n",
    "\n",
    "            train_list[\"outputs\"] += outputs.tolist()\n",
    "            train_list[\"dids\"] += dids.tolist()\n",
    "            train_list[\"targets\"] += targets.tolist()\n",
    "    \n",
    "        # print(f'Epoch {epoch + 1}/{epochs}, Loss: {train_loss/len(train_loader)}, Accuracy: {100.*correct/total}%')\n",
    "        get_final_perf(train_list)\n",
    "        \n",
    "        model.eval()\n",
    "        val_loss = 0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        with torch.no_grad():\n",
    "            total_predicted = []\n",
    "            total_label = []\n",
    "            valid_list = defaultdict(list)\n",
    "            for inputs, input_tfidfs, targets, dids in tqdm.tqdm(val_loader):\n",
    "                inputs, input_tfidfs, targets = inputs.to(device), input_tfidfs.to(device), targets.to(device)\n",
    "                outputs = model(inputs, input_tfidfs)\n",
    "                loss = criterion(outputs, targets)\n",
    "                val_loss += loss.item()\n",
    "                _, predicted = torch.max(outputs, 1)\n",
    "                total += targets.size(0)\n",
    "                correct += predicted.eq(targets).sum().item()\n",
    "\n",
    "                total_predicted += list(predicted)\n",
    "                total_label += list(targets)\n",
    "\n",
    "                valid_list[\"outputs\"] += outputs.tolist()\n",
    "                valid_list[\"dids\"] += dids.tolist()\n",
    "                valid_list[\"targets\"] += targets.tolist()\n",
    "        \n",
    "        # f1 = f1_score(total_label, total_predicted, average='macro')\n",
    "        # print(f'Validation Loss: {val_loss/len(val_loader)}, Accuracy: {100.*correct/total}%, f1 score is {f1}')\n",
    "        get_final_perf(valid_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "38b789c8-0cf8-4e0d-897d-b42c874d11ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 7246/7246 [23:32<00:00,  5.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: 0.8599357142857142, f1 score: 0.8058615301435206\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 3118/3118 [04:45<00:00, 10.93it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: 0.9118166666666667, f1 score: 0.8886402926857556\n"
     ]
    }
   ],
   "source": [
    "train_model(model, train_loader, val_loader, criterion, optimizer, epochs=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e60fa936-d26b-45e5-8080-5801fd81c36b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 7246/7246 [23:33<00:00,  5.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: 0.9231214285714285, f1 score: 0.9047937701919514\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 3118/3118 [04:46<00:00, 10.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: 0.9241, f1 score: 0.9065203823662211\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 7246/7246 [23:09<00:00,  5.22it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: 0.9340642857142857, f1 score: 0.9199154210040581\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 3118/3118 [04:29<00:00, 11.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: 0.9281, f1 score: 0.913164927162792\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 7246/7246 [22:45<00:00,  5.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: 0.9402928571428572, f1 score: 0.9287709887470258\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 82%|████████████████████████████████       | 2566/3118 [03:51<00:47, 11.58it/s]"
     ]
    }
   ],
   "source": [
    "train_model(model, train_loader, val_loader, criterion, optimizer, epochs=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cefe3c44-ea12-4b15-ac3f-a6eaf1e334c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_model(model, train_loader, val_loader, criterion, optimizer, epochs=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b0d5350-dc83-41f7-a4a1-33b72dcae369",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_model(model, train_loader, val_loader, criterion, optimizer, epochs=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19e6ef7b-2814-44f5-b825-49f11c901b48",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_model(model, train_loader, val_loader, criterion, optimizer, epochs=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce21dec8-688d-48c1-956f-00a41c1c9148",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "591f34b1-e2fe-4756-a3e6-b02e7f890701",
   "metadata": {},
   "source": [
    "# 保存模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c84a5ca3-21e1-466c-aeff-bbf6a4b94868",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_pickle_object(model, create_trained_models_path(\"ori_9041-dev_tfidf_lenlim.pkl\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5564b20-6245-4169-a865-77f7e2ba798b",
   "metadata": {},
   "outputs": [],
   "source": [
    "run_finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0d4faa8-e796-4513-933a-0d7628719b76",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "089fc72f-d6f6-4c6d-b5a5-1d4065a61491",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ee9ed8b-e404-4f4d-9701-2fdb51f715da",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e28f73fe-c05f-45fb-ac86-437d8adad664",
   "metadata": {},
   "source": [
    "# 预测一下试试"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06df6c73-b1e2-4382-a0a7-78fc36ca3d05",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 这里还要改一下的哦。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "0a123afe-2fe4-4f8a-903b-1b24f4810a7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 1563/1563 [02:22<00:00, 10.97it/s]\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    total_predicted = []\n",
    "    for inputs, input_tfidfs, targets in tqdm.tqdm(test_loader):\n",
    "        outputs = model(inputs, input_tfidfs)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        total_predicted += list(predicted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "706d090f-a3bc-45f4-81f2-e85fef155296",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50000"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "oot_rst = [int(x) for x in total_predicted]\n",
    "len(oot_rst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "b7bd7c9e-be76-4c3e-89c7-beae8a4ccf90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df.to_csv(\"/Users/minkexiu/Downloads/GitHub/Tianchi_NLPNewsClassification/preprocessedData/rst-20250113_1.csv\", index=False)\n",
      "data saved.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'/Users/minkexiu/Downloads/GitHub/Tianchi_NLPNewsClassification/preprocessedData/rst-20250113_1.csv'"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "store_data_to_newbasepath(pd.DataFrame({\"label\": oot_rst}), \"rst-20250119_1\", fmt=\"csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b76eb7f-745a-4d6f-907f-a738d7348734",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8121230d-20e4-4ffe-9d6a-502ddd3667ea",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06a4bcf3-95b4-4d7f-8a76-689d3de84dc4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3db65fc-dd7b-4ca6-a420-bbe43e95b9b6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
