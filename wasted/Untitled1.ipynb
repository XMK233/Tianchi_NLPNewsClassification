{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "aa5de4dc-0532-4abb-baa5-7d3b9bd1c1ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "storage dir: /Users/minkexiu/Downloads/GitHub/Tianchi_NLPNewsClassification\n",
      "code dir: /Users/minkexiu/Documents/GitHub/Tianchi_NLPNewsClassification \n",
      "\n",
      "20 23 15\n",
      "先天八卦数: 1乾, 2兑, 3离, 4震, 5巽, 6坎, 7艮, 8坤\n",
      "本卦上：4 本卦下：7 变爻：3\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>雷山小过</th>\n",
       "      <th>泽风大过</th>\n",
       "      <th>雷地豫</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>上卦</th>\n",
       "      <td>☳震木</td>\n",
       "      <td>☱兑金</td>\n",
       "      <td>☳震木</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>下卦</th>\n",
       "      <td>☶艮土</td>\n",
       "      <td>☴巽木</td>\n",
       "      <td>☷坤土</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   雷山小过 泽风大过  雷地豫\n",
       "上卦  ☳震木  ☱兑金  ☳震木\n",
       "下卦  ☶艮土  ☴巽木  ☷坤土"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12 21 1 子时\n",
      "先天八卦数: 1乾, 2兑, 3离, 4震, 5巽, 6坎, 7艮, 8坤\n",
      "本卦上：4 本卦下：5 变爻：1\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>雷风恒</th>\n",
       "      <th>泽天夬</th>\n",
       "      <th>雷天大壮</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>上卦</th>\n",
       "      <td>☳震木</td>\n",
       "      <td>☱兑金</td>\n",
       "      <td>☳震木</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>下卦</th>\n",
       "      <td>☴巽木</td>\n",
       "      <td>☰乾金</td>\n",
       "      <td>☰乾金</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    雷风恒  泽天夬 雷天大壮\n",
       "上卦  ☳震木  ☱兑金  ☳震木\n",
       "下卦  ☴巽木  ☰乾金  ☰乾金"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import sys\n",
    "from kaitoupao import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7fcb0dda-0b91-4e1a-8073-6d03b4806d2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, TensorDataset, Dataset\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from sklearn.metrics import f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d4b41aa3-fa1a-412a-abaa-6ab55b842c55",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1ca6cb31-a50e-45de-9db9-3410828cc819",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"mps\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b93f2810-c117-42f3-b78f-08c5dd8351b1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "ed10bbd0-9c5f-464e-bfa0-f428616b9b20",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 2., 3., 4., 5.]])"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.tensor([1,2,3,4,5], dtype=torch.float32).unsqueeze(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2a7e259-e958-4cb5-9a0a-b1e519a82806",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33c0bfd2-1db1-4f00-bead-f5dfa2ebc9e6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cabe918-a9e2-418e-9710-b80faaea95c0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "73717df7-9e4b-41b5-8ac3-2022b2cd90db",
   "metadata": {},
   "outputs": [],
   "source": [
    "t1 = torch.tensor([[1,2,3,4,5], [6,7,8,9,10], [11,12,13,14,15]], dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "bf666846-fa00-4118-97f5-c8e5e997724a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1.,  2.,  3.,  4.,  5.],\n",
       "        [ 6.,  7.,  8.,  9., 10.],\n",
       "        [11., 12., 13., 14., 15.]])"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "7fa8e5cd-2413-48dc-85de-d70f37d41c0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "l1 = nn.Linear(3, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "fd94ec78-3858-4f70-a6a8-98a6842f1dd4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-1.6502, -1.5901, -1.5300, -1.4700, -1.4099],\n",
       "       grad_fn=<SqueezeBackward0>)"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l1(t1.permute(1,0)).permute(1,0).squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "c6045cfd-dac7-46b5-9787-04a5153bc8b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[ 0.4533, -0.4253,  0.0321]], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([0.0955], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "for prm in l1.parameters():\n",
    "    print(prm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "6b59d84c-c8f7-4cb2-bcdb-30608a124e18",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-1.6502, -1.5901, -1.5300, -1.4700, -1.4099],\n",
       "        [-1.6502, -1.5901, -1.5300, -1.4700, -1.4099],\n",
       "        [-1.6502, -1.5901, -1.5300, -1.4700, -1.4099],\n",
       "        [-1.6502, -1.5901, -1.5300, -1.4700, -1.4099]], grad_fn=<CatBackward0>)"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cat([\n",
    "    l1(t1.permute(1,0)).permute(1,0),\n",
    "    l1(t1.permute(1,0)).permute(1,0),\n",
    "    l1(t1.permute(1,0)).permute(1,0),\n",
    "    l1(t1.permute(1,0)).permute(1,0)\n",
    "],dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26dc5c66-86e1-4367-950f-d832c4a187df",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f718362-3557-4c35-adab-c50b24985538",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17e9d3c7-b43c-4b4e-b72a-c14271874610",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b969fb6-c977-410d-8c0f-7423953a0218",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccde09d8-1761-4069-9287-30ca8d5b49ff",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c76fa127-a2a4-40c4-9ad0-541d705c8bf6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf4b0825-974a-43be-8891-e2eb920ca0be",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8100601-43ab-48ec-aa3a-76ba4aac0854",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50bb84c5-e998-43df-9b3c-57d5cdd00a04",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e49450a4-8b12-4c5a-be0a-4b636bbeeea7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "08f6915f-35e2-4f0a-9be1-089f8714d3d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "from transformers import AdamW\n",
    "from torch.utils.data import Dataset\n",
    "import pandas as pd\n",
    "import torch\n",
    "from transformers import BertConfig, BertForSequenceClassification\n",
    "from transformers import BertTokenizer\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b8caa95-0d79-46f6-908f-c69d0d82631a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "01f45d9f-ad90-45c5-a281-5bee52eb65ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer(vocab_file=\"./untitled.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "87cf5a43-eb8d-4b5e-994e-b3588f3002a1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['[UNK]']"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.tokenize(\"this\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f28a4a47-df1a-4955-bdef-279be849e21a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4646bc69-7948-4775-a14a-c78c9519ad2a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "bae0b2a7-e418-438e-8051-d501d4b9e756",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertConfig\n",
    "from transformers import BertForSequenceClassification\n",
    "\n",
    "config = BertConfig(\n",
    "vocab_size=100,\n",
    "hidden_size=512,\n",
    "num_hidden_layers=6,\n",
    "num_attention_heads=8,\n",
    "intermediate_size=2048,\n",
    "max_position_embeddings=512,\n",
    "num_labels=5\n",
    ")\n",
    "\n",
    "model = BertForSequenceClassification(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "d84fd7c4-9fff-42b0-ad6f-f57595176d0b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[0;31mSignature:\u001b[0m\n",
       "\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0minput_ids\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mattention_mask\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mtoken_type_ids\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mposition_ids\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mhead_mask\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0minputs_embeds\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mlabels\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0moutput_attentions\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbool\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0moutput_hidden_states\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbool\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mreturn_dict\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbool\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mTuple\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtransformers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodeling_outputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSequenceClassifierOutput\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
       "\u001b[0;31mDocstring:\u001b[0m\n",
       "The [`BertForSequenceClassification`] forward method, overrides the `__call__` special method.\n",
       "\n",
       "<Tip>\n",
       "\n",
       "Although the recipe for forward pass needs to be defined within this function, one should call the [`Module`]\n",
       "instance afterwards instead of this since the former takes care of running the pre and post processing steps while\n",
       "the latter silently ignores them.\n",
       "\n",
       "</Tip>\n",
       "\n",
       "Args:\n",
       "    input_ids (`torch.LongTensor` of shape `(batch_size, sequence_length)`):\n",
       "        Indices of input sequence tokens in the vocabulary.\n",
       "\n",
       "        Indices can be obtained using [`AutoTokenizer`]. See [`PreTrainedTokenizer.encode`] and\n",
       "        [`PreTrainedTokenizer.__call__`] for details.\n",
       "\n",
       "        [What are input IDs?](../glossary#input-ids)\n",
       "    attention_mask (`torch.FloatTensor` of shape `(batch_size, sequence_length)`or `(batch_size, sequence_length, target_length)`, *optional*):\n",
       "        Mask to avoid performing attention on padding token indices. Mask values selected in `[0, 1]`:\n",
       "\n",
       "        - 1 for tokens that are **not masked**,\n",
       "        - 0 for tokens that are **masked**.\n",
       "\n",
       "        [What are attention masks?](../glossary#attention-mask)\n",
       "    token_type_ids (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n",
       "        Segment token indices to indicate first and second portions of the inputs. Indices are selected in `[0,\n",
       "        1]`:\n",
       "\n",
       "        - 0 corresponds to a *sentence A* token,\n",
       "        - 1 corresponds to a *sentence B* token.\n",
       "\n",
       "        [What are token type IDs?](../glossary#token-type-ids)\n",
       "    position_ids (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n",
       "        Indices of positions of each input sequence tokens in the position embeddings. Selected in the range `[0,\n",
       "        config.max_position_embeddings - 1]`.\n",
       "\n",
       "        [What are position IDs?](../glossary#position-ids)\n",
       "    head_mask (`torch.FloatTensor` of shape `(num_heads,)` or `(num_layers, num_heads)`, *optional*):\n",
       "        Mask to nullify selected heads of the self-attention modules. Mask values selected in `[0, 1]`:\n",
       "\n",
       "        - 1 indicates the head is **not masked**,\n",
       "        - 0 indicates the head is **masked**.\n",
       "\n",
       "    inputs_embeds (`torch.FloatTensor` of shape `(batch_size, sequence_length, hidden_size)`, *optional*):\n",
       "        Optionally, instead of passing `input_ids` you can choose to directly pass an embedded representation. This\n",
       "        is useful if you want more control over how to convert `input_ids` indices into associated vectors than the\n",
       "        model's internal embedding lookup matrix.\n",
       "    output_attentions (`bool`, *optional*):\n",
       "        Whether or not to return the attentions tensors of all attention layers. See `attentions` under returned\n",
       "        tensors for more detail.\n",
       "    output_hidden_states (`bool`, *optional*):\n",
       "        Whether or not to return the hidden states of all layers. See `hidden_states` under returned tensors for\n",
       "        more detail.\n",
       "    return_dict (`bool`, *optional*):\n",
       "        Whether or not to return a [`~utils.ModelOutput`] instead of a plain tuple.\n",
       "\n",
       "    labels (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\n",
       "        Labels for computing the sequence classification/regression loss. Indices should be in `[0, ...,\n",
       "        config.num_labels - 1]`. If `config.num_labels == 1` a regression loss is computed (Mean-Square loss), If\n",
       "        `config.num_labels > 1` a classification loss is computed (Cross-Entropy).\n",
       "    \n",
       "Returns:\n",
       "    [`transformers.modeling_outputs.SequenceClassifierOutput`] or `tuple(torch.FloatTensor)`: A [`transformers.modeling_outputs.SequenceClassifierOutput`] or a tuple of\n",
       "    `torch.FloatTensor` (if `return_dict=False` is passed or when `config.return_dict=False`) comprising various\n",
       "    elements depending on the configuration ([`BertConfig`]) and inputs.\n",
       "\n",
       "    - **loss** (`torch.FloatTensor` of shape `(1,)`, *optional*, returned when `labels` is provided) -- Classification (or regression if config.num_labels==1) loss.\n",
       "    - **logits** (`torch.FloatTensor` of shape `(batch_size, config.num_labels)`) -- Classification (or regression if config.num_labels==1) scores (before SoftMax).\n",
       "    - **hidden_states** (`tuple(torch.FloatTensor)`, *optional*, returned when `output_hidden_states=True` is passed or when `config.output_hidden_states=True`) -- Tuple of `torch.FloatTensor` (one for the output of the embeddings, if the model has an embedding layer, +\n",
       "      one for the output of each layer) of shape `(batch_size, sequence_length, hidden_size)`.\n",
       "\n",
       "      Hidden-states of the model at the output of each layer plus the optional initial embedding outputs.\n",
       "    - **attentions** (`tuple(torch.FloatTensor)`, *optional*, returned when `output_attentions=True` is passed or when `config.output_attentions=True`) -- Tuple of `torch.FloatTensor` (one for each layer) of shape `(batch_size, num_heads, sequence_length,\n",
       "      sequence_length)`.\n",
       "\n",
       "      Attentions weights after the attention softmax, used to compute the weighted average in the self-attention\n",
       "      heads.\n",
       "\n",
       "Example of single-label classification:\n",
       "\n",
       "```python\n",
       ">>> import torch\n",
       ">>> from transformers import AutoTokenizer, BertForSequenceClassification\n",
       "\n",
       ">>> tokenizer = AutoTokenizer.from_pretrained(\"textattack/bert-base-uncased-yelp-polarity\")\n",
       ">>> model = BertForSequenceClassification.from_pretrained(\"textattack/bert-base-uncased-yelp-polarity\")\n",
       "\n",
       ">>> inputs = tokenizer(\"Hello, my dog is cute\", return_tensors=\"pt\")\n",
       "\n",
       ">>> with torch.no_grad():\n",
       "...     logits = model(**inputs).logits\n",
       "\n",
       ">>> predicted_class_id = logits.argmax().item()\n",
       ">>> model.config.id2label[predicted_class_id]\n",
       "'LABEL_1'\n",
       "\n",
       ">>> # To train a model on `num_labels` classes, you can pass `num_labels=num_labels` to `.from_pretrained(...)`\n",
       ">>> num_labels = len(model.config.id2label)\n",
       ">>> model = BertForSequenceClassification.from_pretrained(\"textattack/bert-base-uncased-yelp-polarity\", num_labels=num_labels)\n",
       "\n",
       ">>> labels = torch.tensor([1])\n",
       ">>> loss = model(**inputs, labels=labels).loss\n",
       ">>> round(loss.item(), 2)\n",
       "0.01\n",
       "```\n",
       "\n",
       "Example of multi-label classification:\n",
       "\n",
       "```python\n",
       ">>> import torch\n",
       ">>> from transformers import AutoTokenizer, BertForSequenceClassification\n",
       "\n",
       ">>> tokenizer = AutoTokenizer.from_pretrained(\"textattack/bert-base-uncased-yelp-polarity\")\n",
       ">>> model = BertForSequenceClassification.from_pretrained(\"textattack/bert-base-uncased-yelp-polarity\", problem_type=\"multi_label_classification\")\n",
       "\n",
       ">>> inputs = tokenizer(\"Hello, my dog is cute\", return_tensors=\"pt\")\n",
       "\n",
       ">>> with torch.no_grad():\n",
       "...     logits = model(**inputs).logits\n",
       "\n",
       ">>> predicted_class_ids = torch.arange(0, logits.shape[-1])[torch.sigmoid(logits).squeeze(dim=0) > 0.5]\n",
       "\n",
       ">>> # To train a model on `num_labels` classes, you can pass `num_labels=num_labels` to `.from_pretrained(...)`\n",
       ">>> num_labels = len(model.config.id2label)\n",
       ">>> model = BertForSequenceClassification.from_pretrained(\n",
       "...     \"textattack/bert-base-uncased-yelp-polarity\", num_labels=num_labels, problem_type=\"multi_label_classification\"\n",
       "... )\n",
       "\n",
       ">>> labels = torch.sum(\n",
       "...     torch.nn.functional.one_hot(predicted_class_ids[None, :].clone(), num_classes=num_labels), dim=1\n",
       "... ).to(torch.float)\n",
       ">>> loss = model(**inputs, labels=labels).loss\n",
       "```\n",
       "\u001b[0;31mFile:\u001b[0m      /opt/anaconda3/envs/ml12/lib/python3.12/site-packages/transformers/models/bert/modeling_bert.py\n",
       "\u001b[0;31mType:\u001b[0m      method"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model.forward?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "7e6612cf-e0af-44f1-b952-0b66e05d285f",
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs = model(input_ids=torch.tensor([[1,2,3,4,5]], dtype=torch.long))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "392dc230-2389-439e-ad6d-ee06f520e225",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.1212, -0.0173, -0.4758,  0.0187,  0.3480]],\n",
       "       grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs.logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8f87f9e-1e24-409e-b517-750a7adc0bb1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "927844d9-c25e-4635-80b8-1dc4e8dfe65a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4451d876-253d-4c39-80ab-cc45e6fcd61a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a670f57a-ad61-4166-9e5c-9c3e96d01415",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88cfd468-c513-4e5e-8e41-dcfbc0de2986",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "838e59e1-64db-425e-a49f-da03a0dda219",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e79bd1d-8a18-46d4-94a6-3351cfed8807",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10c987b6-d9e8-47d6-b976-93b7c76f53f3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
