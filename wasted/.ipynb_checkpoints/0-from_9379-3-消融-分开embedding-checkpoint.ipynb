{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8d8114b5-ffbc-4b0a-9e85-4c007576f1c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "storage dir: /mnt/d/forCoding_data/Tianchi_NLPNewsClassification\n",
      "code dir: /mnt/d/forCoding_code/Tianchi_NLPNewsClassification \n",
      "\n",
      "21 09 47\n",
      "先天八卦数: 1乾, 2兑, 3离, 4震, 5巽, 6坎, 7艮, 8坤\n",
      "本卦上：5 本卦下：1 变爻：5\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>风天小畜</th>\n",
       "      <th>火泽睽</th>\n",
       "      <th>山天大畜</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>上卦</th>\n",
       "      <td>☴巽木</td>\n",
       "      <td>☲离火</td>\n",
       "      <td>☶艮土</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>下卦</th>\n",
       "      <td>☰乾金</td>\n",
       "      <td>☱兑金</td>\n",
       "      <td>☰乾金</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   风天小畜  火泽睽 山天大畜\n",
       "上卦  ☴巽木  ☲离火  ☶艮土\n",
       "下卦  ☰乾金  ☱兑金  ☰乾金"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "01 24 6 巳时\n",
      "先天八卦数: 1乾, 2兑, 3离, 4震, 5巽, 6坎, 7艮, 8坤\n",
      "本卦上：1 本卦下：8 变爻：6\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>天地否</th>\n",
       "      <th>风山渐</th>\n",
       "      <th>泽地萃</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>上卦</th>\n",
       "      <td>☰乾金</td>\n",
       "      <td>☴巽木</td>\n",
       "      <td>☱兑金</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>下卦</th>\n",
       "      <td>☷坤土</td>\n",
       "      <td>☶艮土</td>\n",
       "      <td>☷坤土</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    天地否  风山渐  泽地萃\n",
       "上卦  ☰乾金  ☴巽木  ☱兑金\n",
       "下卦  ☷坤土  ☶艮土  ☷坤土"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import sys\n",
    "from kaitoupao_wsl import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4d85fbf8-6949-4d26-a614-27ae9f1ecab3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, TensorDataset, Dataset\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from sklearn.metrics import f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6605d765-b333-41cc-9da7-928a2eb80d61",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "319b2f47-d25d-4943-b794-5865c4a8ff25",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "81e0e79e-4733-4950-8e77-0598ad139549",
   "metadata": {},
   "outputs": [],
   "source": [
    "scheme_type = \"20250221_1\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc63e99d-800a-4cfb-a8a5-379ca1c1e042",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "151910d0-6c8d-423f-87bc-538f48073940",
   "metadata": {},
   "source": [
    "# 加载训练集和测试集，将全量字符列表给它弄出来"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "80519151-9c80-4bbe-8967-ca9b9d9952af",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train = pd.read_csv(create_originalData_path(\"train_set.csv\"), sep=\"\\t\", nrows=None)#.sample(1000) , nrows=1000\n",
    "data_test = pd.read_csv(create_originalData_path(\"test_a.csv\"), sep=\"\\t\", nrows=None)#.sample(1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7003ef74-4ad9-435f-9234-516e9f608f2a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((200000, 2), (50000, 1))"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_train.shape, data_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d37fff12-c89e-4e84-9a9d-89064b711b94",
   "metadata": {},
   "outputs": [],
   "source": [
    "type_of_class = data_train.label.nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "808d37eb-c13a-4d59-923d-cd5aa4a6740b",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, valid_data = train_test_split(data_train, test_size=0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1bd3acc9-b6fe-4b86-a846-64ba4cd2e5ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_labels = torch.tensor(train_data.label.to_list(), dtype=torch.long)\n",
    "valid_labels = torch.tensor(valid_data.label.to_list(), dtype=torch.long)\n",
    "test_labels = torch.tensor([-1 for x in range(data_test.shape[0])], dtype=torch.long) ## test_labels的label是假的。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd8aedbd-696c-457d-894f-8acc51bcdc61",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5784b333-7a55-4d5d-9a71-8d2704965e70",
   "metadata": {},
   "source": [
    "## 使用TF-IDF提取特征"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "773f5b4b-bb56-4aed-aea4-19db9e9391c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "tfidf_train_features = tfidf_vectorizer.fit_transform(train_data['text'])\n",
    "tfidf_valid_features = tfidf_vectorizer.transform(valid_data['text'])\n",
    "tfidf_test_features = tfidf_vectorizer.transform(data_test['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "318aa667-e44a-46d6-9f60-af2781806313",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_features = torch.tensor(tfidf_train_features.toarray(), dtype=torch.float32)\n",
    "valid_features = torch.tensor(tfidf_valid_features.toarray(), dtype=torch.float32)\n",
    "test_features = torch.tensor(tfidf_test_features.toarray(), dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "95376078-9014-4cef-8dd1-f3a142644858",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6695"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc_input_dim = train_features.shape[1]\n",
    "sc_input_dim"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27d9d9c0-a3f6-46a0-b88b-5a0399cc88ec",
   "metadata": {},
   "source": [
    "## 创建适合于语言序列的数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "359de0fd-bc05-47e1-a624-a6d23e86b4b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 下载数据并进行预处理\n",
    "vocab_size = 8000  # 只考虑前 20k 词汇\n",
    "maxlen = 800  # 只考虑每条评论的前 200 个词"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "46ac102a-e2fb-478a-b2c6-6998ed02866f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_seq_str_2_int(seq, len_lim = maxlen):\n",
    "    rst = [int(wd) for idx, wd in enumerate(seq.strip().split()) if idx < len_lim]\n",
    "    return rst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f609e647-eea4-4674-8fe9-3c39f4a604c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = [torch.tensor(preprocess_seq_str_2_int(seq), dtype=torch.long) for seq in train_data.text]\n",
    "x_valid = [torch.tensor(preprocess_seq_str_2_int(seq), dtype=torch.long) for seq in valid_data.text]\n",
    "x_test = [torch.tensor(preprocess_seq_str_2_int(seq), dtype=torch.long) for seq in data_test.text]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "582fb80e-de88-4bc9-907b-d7b49b8e5753",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = pad_sequence(x_train, batch_first=True, padding_value=0)\n",
    "x_valid = pad_sequence(x_valid, batch_first=True, padding_value=0)\n",
    "x_test = pad_sequence(x_test, batch_first=True, padding_value=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6873e519-4b58-4ed5-90d6-b56bb0419929",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0c7691fe-777d-422a-8e84-015c3654d88e",
   "metadata": {},
   "source": [
    "## 准备数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6b898697-cfcc-4f89-b334-6abd66ec6270",
   "metadata": {},
   "outputs": [],
   "source": [
    "batchsize = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "91d67ebd-ba91-494f-b148-d3945a95b474",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyData(Dataset):\n",
    "    def __init__(\n",
    "        self, \n",
    "        ori_data, tfidf_feats, label,\n",
    "    ):\n",
    "        self.ori_data = ori_data\n",
    "        self.tfidf_feats = tfidf_feats\n",
    "        self.label = label\n",
    " \n",
    "    def __len__(self):\n",
    "        return len(self.ori_data)\n",
    " \n",
    "    def __getitem__(self, idx):\n",
    "        tuple_ = (\n",
    "            self.ori_data[idx], \n",
    "            self.tfidf_feats[idx], \n",
    "            self.label[idx]\n",
    "        )\n",
    "        return tuple_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "bec272ff-950c-4517-8702-40349394029c",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(MyData(x_train, train_features, train_labels), batch_size=batchsize, shuffle=True,)\n",
    "val_loader = DataLoader(MyData(x_valid, valid_features, valid_labels), batch_size=batchsize, shuffle=True,)\n",
    "test_loader = DataLoader(MyData(x_test, test_features, test_labels), batch_size=batchsize, shuffle=False,) ## 这个不要shuffle，否则传到oj上面去就GG了。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42313977-935c-499d-815d-2886154bd4a1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5ae440d3-e3d2-4907-ac77-0d97962c576a",
   "metadata": {},
   "source": [
    "# Scheme 1: base\n",
    "\n",
    "并行使用CNN+BiLSTM+Transformer：\n",
    "* CNN捕捉局部n-gram特征（kernel_size=3,5,7）\n",
    "* BiLSTM捕获长距离时序依赖\n",
    "* Transformer处理全局关系"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ec6908fc-0085-48d6-8876-536ac58156b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "scheme_type_here = f\"{scheme_type}__base\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a123d3a0-63ad-4991-af25-51fceabb302a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, embed_dim, num_heads, ff_dim, dropout=0.1):\n",
    "        super(TransformerBlock, self).__init__()\n",
    "        self.att = nn.MultiheadAttention(embed_dim, num_heads)\n",
    "        self.ffn = nn.Sequential(\n",
    "            nn.Linear(embed_dim, ff_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(ff_dim, embed_dim)\n",
    "        )\n",
    "        self.layernorm1 = nn.LayerNorm(embed_dim)\n",
    "        self.layernorm2 = nn.LayerNorm(embed_dim)\n",
    "        self.dropout1 = nn.Dropout(dropout)\n",
    "        self.dropout2 = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        attn_output, _ = self.att(x, x, x)\n",
    "        attn_output = self.dropout1(attn_output)\n",
    "        out1 = self.layernorm1(x + attn_output)\n",
    "        ffn_output = self.ffn(out1)\n",
    "        ffn_output = self.dropout2(ffn_output)\n",
    "        return self.layernorm2(out1 + ffn_output)\n",
    "\n",
    "class TokenAndPositionEmbedding(nn.Module):\n",
    "    def __init__(self, maxlen, vocab_size, embed_dim):\n",
    "        super(TokenAndPositionEmbedding, self).__init__()\n",
    "        self.pos_emb = nn.Embedding(maxlen, embed_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        maxlen = x.size(1)\n",
    "        positions = torch.arange(0, maxlen, device=x.device).unsqueeze(0).expand(x.size(0), x.size(1)) # torch.arange(0, maxlen, device=x.device).unsqueeze(0).expand_as(x)\n",
    "        # print(positions.shape)\n",
    "        return x + self.pos_emb(positions)\n",
    "\n",
    "class TransformerModel(nn.Module):\n",
    "    def __init__(self, maxlen, vocab_size, embed_dim, num_heads, ff_dim):\n",
    "        super(TransformerModel, self).__init__()\n",
    "        self.embedding_layer = TokenAndPositionEmbedding(maxlen, vocab_size, embed_dim)\n",
    "        self.transformer_block = TransformerBlock(embed_dim, num_heads, ff_dim)\n",
    "        self.global_avg_pool = nn.AdaptiveAvgPool1d(1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.embedding_layer(x).transpose(0, 1)  # Transformer expects (seq_len, batch_size, embed_dim)\n",
    "        x = self.transformer_block(x)\n",
    "        x = x.transpose(0, 1)\n",
    "        x = self.global_avg_pool(x.permute(0, 2, 1)).squeeze(-1)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c9ceafa0-511d-44fa-9663-3801e5541c90",
   "metadata": {},
   "outputs": [],
   "source": [
    "## TextCNN：捕捉短时间的关系\n",
    "\n",
    "# https://blog.51cto.com/u_15764210/6844118\n",
    "class GlobalMaxPool1d(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(GlobalMaxPool1d, self).__init__()\n",
    "    def forward(self, x):\n",
    "        return F.max_pool1d(x, kernel_size=x.shape[2]) # shape: (batch_size, channel, 1)\n",
    "        \n",
    "class TextCNN(nn.Module):\n",
    "    def __init__(\n",
    "        self, \n",
    "        embedding_dim=128, \n",
    "        kernel_sizes=[3, 4, 5, 6], num_channels=[256, 256, 256, 256], \n",
    "    ):\n",
    "        '''\n",
    "        ：param num_classes：输出维度（类别数num_Classes）\n",
    "        ：param num_embeddings: size of the dictionary of embeddings，词典的大小（vocab_size），当num_embeddings<O，模型会去除embedding层\n",
    "        ：param embedding_dim: the size of each embedding vector，词向量特征长度\n",
    "        ：param kernel_sizes: CNN层卷积核大小\n",
    "        ：param num_channels: CNN层卷积核通道数\n",
    "        : return:\n",
    "        '''\n",
    "        assert len(kernel_sizes) == len(num_channels), \"len(kernel_sizes) should be equal to len(num_channels)\"\n",
    "        super(TextCNN, self).__init__()\n",
    "        # self.num_classes = num_classes\n",
    "    \n",
    "        # 卷积层\n",
    "        self.cnn_layers = nn.ModuleList() # 创建多个一维卷积层\n",
    "        for c, k in zip(num_channels, kernel_sizes):\n",
    "            cnn = nn.Sequential(\n",
    "                nn.Conv1d(\n",
    "                    in_channels=embedding_dim,\n",
    "                    out_channels=c, \n",
    "                    kernel_size=k\n",
    "                ),\n",
    "                nn.BatchNorm1d(c),\n",
    "                nn.ReLU(inplace=True),\n",
    "            )\n",
    "            self.cnn_layers.append(cnn)\n",
    "        # 最大池化层\n",
    "        self.pool = GlobalMaxPool1d()\n",
    "            \n",
    "    def forward(self, input_):\n",
    "        '''\n",
    "        :param input: (batch_size, context_size, embedding_size(in_channels))\n",
    "        :return:\n",
    "        '''\n",
    "        input_ = input_.permute(0, 2, 1)\n",
    "        y = []\n",
    "        for layer in self.cnn_layers:\n",
    "            x = layer(input_)\n",
    "            x = self.pool(x).squeeze(-1)\n",
    "            y.append(x)\n",
    "        y = torch.cat(y, dim=1)\n",
    "        return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "305fa83b-d0e7-4bb6-99a2-223856077695",
   "metadata": {},
   "outputs": [],
   "source": [
    "# BiLSTM \n",
    "class LSTMClassifier(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, bidirectional=False):\n",
    "        super(LSTMClassifier, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.bi_dir = bidirectional\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True, bidirectional=bidirectional)\n",
    "\n",
    "    def forward(self, x):\n",
    "        h0 = torch.zeros(self.num_layers *(2 if self.bi_dir else 1), x.size(0), self.hidden_size).to(x.device)\n",
    "        c0 = torch.zeros(self.num_layers *(2 if self.bi_dir else 1), x.size(0), self.hidden_size).to(x.device)\n",
    "        out, _ = self.lstm(x, (h0, c0))\n",
    "        return out[:, -1, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "e6a1e31c-1b93-4476-ac9f-f36d6f23b622",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 多层神经网络：\n",
    "class SentimentClassifier(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super().__init__()\n",
    "        self.fc = nn.Linear(input_dim, 32)\n",
    "        self.dropout1 = nn.Dropout(0.2)\n",
    "        self.dense1 = nn.Linear(32, 32)\n",
    "        self.dropout2 = nn.Dropout(0.2)\n",
    "        self.dense2 = nn.Linear(32, output_dim)\n",
    " \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc(x))\n",
    "        x = self.dropout1(x)\n",
    "        x = F.relu(self.dense1(x))\n",
    "        x = self.dropout2(x)\n",
    "        x = self.dense2(x)        \n",
    "        return x\n",
    "\n",
    "# # 单层神经网络：\n",
    "# class SentimentClassifier(nn.Module):\n",
    "#     def __init__(self, input_dim, output_dim):\n",
    "#         super().__init__()\n",
    "#         self.fc = nn.Linear(input_dim, output_dim)\n",
    " \n",
    "#     def forward(self, x):\n",
    "#         return self.fc(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "ac2c7655-9005-4d46-9829-9c1d1c04080b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义模型\n",
    "class FinalModel(nn.Module):\n",
    "    def __init__(self, \n",
    "                 maxlen, vocab_size, embed_dim, num_heads, ff_dim,\n",
    "                tcnn_ks = [3,5,7,10], tcnn_nc = [32,64,64,64],\n",
    "                 lstm_hs = 128, lstm_nlyr = 4, lstm_bd = True\n",
    "                ):\n",
    "        super(FinalModel, self).__init__()\n",
    "        ## emb层\n",
    "        self.token_emb = nn.Embedding(vocab_size, embed_dim)\n",
    "        ## Tsfm部分：\n",
    "        self.tsfm = TransformerModel(\n",
    "            maxlen, \n",
    "            vocab_size, \n",
    "            embed_dim, \n",
    "            num_heads, \n",
    "            ff_dim\n",
    "        ) # embed_dim\n",
    "        ## TextCNN部分：\n",
    "        self.textcnn = TextCNN(\n",
    "            embedding_dim = embed_dim, \n",
    "            kernel_sizes = tcnn_ks,\n",
    "            num_channels = tcnn_nc\n",
    "        ) # sum(tcnn_nc)\n",
    "        ## BiLSTM:\n",
    "        self.lstm = LSTMClassifier(\n",
    "            input_size = embed_dim, \n",
    "            hidden_size = lstm_hs, \n",
    "            num_layers = lstm_nlyr,  \n",
    "            bidirectional = lstm_bd\n",
    "        ) # lstm_hs * (2 if lstm_bd else 1)\n",
    "        self.mix = nn.Sequential(\n",
    "            ## 基于这个数字 embed_dim + sum(tcnn_nc) + lstm_hs * (2 if lstm_bd else 1) ，做一个全连接神经网络吧。\n",
    "            nn.Linear(\n",
    "                embed_dim + sum(tcnn_nc) + lstm_hs * (2 if lstm_bd else 1),\n",
    "                ff_dim\n",
    "            ),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.1),\n",
    "            nn.Linear(ff_dim, ff_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.1),\n",
    "            nn.Linear(ff_dim, type_of_class)\n",
    "        )\n",
    "        self.sc_net = SentimentClassifier(sc_input_dim, type_of_class)\n",
    "        \n",
    "    def forward(self, x, x_tfidf):\n",
    "        x_ori = x\n",
    "        x_emb = self.token_emb(x)\n",
    "        ## Tsfm部分：\n",
    "        x_tsfm = self.tsfm(x_emb) \n",
    "        ## TextCNN部分：\n",
    "        x_tcnn = self.textcnn(x_emb)\n",
    "        ## BiLSTM部分:\n",
    "        x_lstm = self.lstm(x_emb)\n",
    "        ## 综合：\n",
    "        x_cat = torch.cat(\n",
    "            [\n",
    "                x_tsfm, \n",
    "                x_tcnn, x_lstm\n",
    "            ], axis=1\n",
    "        )        \n",
    "        return F.log_softmax(self.mix(x_cat) + self.sc_net(x_tfidf), dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "535f8358-1cc9-4bd9-b93d-04975823d431",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f235d8db-587a-4b35-aad6-8286995d73b2",
   "metadata": {},
   "source": [
    "## 构建模型以及训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "ef3b3ae4-c7db-48aa-9b53-803138555eee",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = FinalModel(maxlen, vocab_size, embed_dim=128, num_heads=8, ff_dim=128)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=0.001)\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "f873c185-c9bc-4c08-9f4d-257022f104a6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FinalModel(\n",
       "  (token_emb): Embedding(8000, 128)\n",
       "  (tsfm): TransformerModel(\n",
       "    (embedding_layer): TokenAndPositionEmbedding(\n",
       "      (pos_emb): Embedding(800, 128)\n",
       "    )\n",
       "    (transformer_block): TransformerBlock(\n",
       "      (att): MultiheadAttention(\n",
       "        (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)\n",
       "      )\n",
       "      (ffn): Sequential(\n",
       "        (0): Linear(in_features=128, out_features=128, bias=True)\n",
       "        (1): ReLU()\n",
       "        (2): Linear(in_features=128, out_features=128, bias=True)\n",
       "      )\n",
       "      (layernorm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "      (layernorm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "      (dropout1): Dropout(p=0.1, inplace=False)\n",
       "      (dropout2): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (global_avg_pool): AdaptiveAvgPool1d(output_size=1)\n",
       "  )\n",
       "  (textcnn): TextCNN(\n",
       "    (cnn_layers): ModuleList(\n",
       "      (0): Sequential(\n",
       "        (0): Conv1d(128, 32, kernel_size=(3,), stride=(1,))\n",
       "        (1): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU(inplace=True)\n",
       "      )\n",
       "      (1): Sequential(\n",
       "        (0): Conv1d(128, 64, kernel_size=(5,), stride=(1,))\n",
       "        (1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU(inplace=True)\n",
       "      )\n",
       "      (2): Sequential(\n",
       "        (0): Conv1d(128, 64, kernel_size=(7,), stride=(1,))\n",
       "        (1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU(inplace=True)\n",
       "      )\n",
       "      (3): Sequential(\n",
       "        (0): Conv1d(128, 64, kernel_size=(10,), stride=(1,))\n",
       "        (1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU(inplace=True)\n",
       "      )\n",
       "    )\n",
       "    (pool): GlobalMaxPool1d()\n",
       "  )\n",
       "  (lstm): LSTMClassifier(\n",
       "    (lstm): LSTM(128, 128, num_layers=4, batch_first=True, bidirectional=True)\n",
       "  )\n",
       "  (mix): Sequential(\n",
       "    (0): Linear(in_features=608, out_features=128, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Dropout(p=0.1, inplace=False)\n",
       "    (3): Linear(in_features=128, out_features=128, bias=True)\n",
       "    (4): ReLU()\n",
       "    (5): Dropout(p=0.1, inplace=False)\n",
       "    (6): Linear(in_features=128, out_features=14, bias=True)\n",
       "  )\n",
       "  (sc_net): SentimentClassifier(\n",
       "    (fc): Linear(in_features=6695, out_features=32, bias=True)\n",
       "    (dropout1): Dropout(p=0.2, inplace=False)\n",
       "    (dense1): Linear(in_features=32, out_features=32, bias=True)\n",
       "    (dropout2): Dropout(p=0.2, inplace=False)\n",
       "    (dense2): Linear(in_features=32, out_features=14, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "be7ce043-bef9-4240-9f2c-4801a077760c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 训练和评估模型\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import shutil\n",
    "\n",
    "def train_model(model, train_loader, val_loader, criterion, optimizer, epochs=2):\n",
    "    # device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    model.to(device)\n",
    "    \n",
    "    log_path = \"runs/Logs\"\n",
    "    if os.path.exists(\n",
    "        os.path.dirname(log_path)\n",
    "    ):\n",
    "        shutil.rmtree(os.path.dirname(log_path))\n",
    "\n",
    "    writer = SummaryWriter(log_dir=log_path)\n",
    "    \n",
    "    total_train_step = 0\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        loss_sum = 0\n",
    "        for inputs, input_tfidfs, targets in tqdm.tqdm(train_loader):\n",
    "            inputs, input_tfidfs, targets = inputs.to(device), input_tfidfs.to(device), targets.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs, input_tfidfs)\n",
    "            loss = criterion(outputs, targets)\n",
    "            loss.requires_grad_(True)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_loss += loss.item()\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            total += targets.size(0)\n",
    "            correct += predicted.eq(targets).sum().item()\n",
    "            \n",
    "            total_train_step += 1\n",
    "            loss_sum += loss.item()\n",
    "            if total_train_step % 100 == 0:\n",
    "                # print(total_train_step//100)\n",
    "                writer.add_scalar(f\"train_loss_detail-{scheme_type_here}\", loss_sum/100, total_train_step//100)\n",
    "                loss_sum = 0\n",
    "                \n",
    "        print(f'Epoch {epoch + 1}/{epochs}, Loss: {train_loss/len(train_loader)}, Accuracy: {100.*correct/total}%')\n",
    "        writer.add_scalar(f\"epoch_loss-{scheme_type_here}\", train_loss/len(train_loader), epoch+1)\n",
    "        \n",
    "        model.eval()\n",
    "        val_loss = 0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        with torch.no_grad():\n",
    "            total_predicted = []\n",
    "            total_label = []\n",
    "            for inputs, input_tfidfs, targets in tqdm.tqdm(val_loader):\n",
    "                inputs, input_tfidfs, targets = inputs.to(device), input_tfidfs.to(device), targets.to(device)\n",
    "                outputs = model(inputs, input_tfidfs)\n",
    "                loss = criterion(outputs, targets)\n",
    "                val_loss += loss.item()\n",
    "                _, predicted = torch.max(outputs, 1)\n",
    "                total += targets.size(0)\n",
    "                correct += predicted.eq(targets).sum().item()\n",
    "\n",
    "                total_predicted += list(predicted.cpu())\n",
    "                total_label += list(targets.cpu())\n",
    "        f1 = f1_score(total_label, total_predicted, average='macro')\n",
    "                \n",
    "        print(f'Validation Loss: {val_loss/len(val_loader)}, Accuracy: {100.*correct/total}%, f1 score is {f1}')\n",
    "    writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "38b789c8-0cf8-4e0d-897d-b42c874d11ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████| 4375/4375 [04:22<00:00, 16.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15, Loss: 0.3906377124946032, Accuracy: 88.15%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████| 1875/1875 [00:41<00:00, 44.84it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 0.228994959337761, Accuracy: 93.02666666666667%, f1 score is 0.9165290498517022\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████| 4375/4375 [04:20<00:00, 16.81it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/15, Loss: 0.20031537963630897, Accuracy: 93.92214285714286%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████| 1875/1875 [00:41<00:00, 45.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 0.20809106399888794, Accuracy: 93.50833333333334%, f1 score is 0.9221683104951861\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████| 4375/4375 [04:19<00:00, 16.83it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/15, Loss: 0.15410339160199676, Accuracy: 95.16214285714285%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████| 1875/1875 [00:41<00:00, 45.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 0.18277862702794373, Accuracy: 94.41%, f1 score is 0.9319527445370067\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████| 4375/4375 [04:19<00:00, 16.83it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/15, Loss: 0.12084832793064415, Accuracy: 96.16214285714285%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████| 1875/1875 [00:41<00:00, 45.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 0.20541956249661744, Accuracy: 94.14166666666667%, f1 score is 0.9290737165845923\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████| 4375/4375 [04:19<00:00, 16.85it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/15, Loss: 0.09360727666479403, Accuracy: 96.97%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████| 1875/1875 [00:41<00:00, 45.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 0.20924845323693006, Accuracy: 94.31833333333333%, f1 score is 0.9322833407628525\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████| 4375/4375 [04:19<00:00, 16.86it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/15, Loss: 0.07289789363950758, Accuracy: 97.59071428571428%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████| 1875/1875 [00:41<00:00, 45.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 0.22137073640283197, Accuracy: 94.26666666666667%, f1 score is 0.9274365931256724\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████| 4375/4375 [04:19<00:00, 16.87it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/15, Loss: 0.05838802612501396, Accuracy: 98.045%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████| 1875/1875 [00:41<00:00, 45.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 0.24911891760854052, Accuracy: 94.16166666666666%, f1 score is 0.9290523147117392\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 14%|███████████                                                                     | 606/4375 [00:35<03:43, 16.83it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[33], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m15\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[32], line 33\u001b[0m, in \u001b[0;36mtrain_model\u001b[0;34m(model, train_loader, val_loader, criterion, optimizer, epochs)\u001b[0m\n\u001b[1;32m     31\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[1;32m     32\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[0;32m---> 33\u001b[0m train_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     34\u001b[0m _, predicted \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mmax(outputs, \u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     35\u001b[0m total \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m targets\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m0\u001b[39m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train_model(model, train_loader, val_loader, criterion, optimizer, epochs=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d981286a-b45e-444d-956d-40015c302121",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "fda2273a-b9be-48bb-b6a7-bbba54681dca",
   "metadata": {},
   "source": [
    "## 保存模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c84a5ca3-21e1-466c-aeff-bbf6a4b94868",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_pickle_object(model, create_trained_models_path(f\"ori_9319-{scheme_type_here}.pkl\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "453e182b-c62a-44fd-afc5-97c4a1495f08",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "cbd22aae-37b9-448c-a466-2337db1b833a",
   "metadata": {},
   "source": [
    "# Scheme 2: 共用基座但是各自投影"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "eb996305-5176-4598-9ded-fa847c05ac07",
   "metadata": {},
   "outputs": [],
   "source": [
    "scheme_type_here = f\"{scheme_type}__com_bench\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "b66f9d87-7b14-425d-b687-884d57224cdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义模型\n",
    "class FinalModel(nn.Module):\n",
    "    def __init__(self, \n",
    "                 maxlen, vocab_size, \n",
    "                 ori_emb_dim = 512,\n",
    "                 embed_dim=128, \n",
    "                 num_heads=8, ff_dim=128,\n",
    "                tcnn_ks = [3,5,7,10], tcnn_nc = [32,64,64,64],\n",
    "                 lstm_hs = 128, lstm_nlyr = 4, lstm_bd = True\n",
    "                ):\n",
    "        super(FinalModel, self).__init__()\n",
    "        ## emb层\n",
    "        self.token_emb = nn.Embedding(vocab_size, ori_emb_dim)\n",
    "        ## Tsfm部分：\n",
    "        self.proj_trans = nn.Linear(ori_emb_dim, embed_dim)\n",
    "        self.tsfm = TransformerModel(\n",
    "            maxlen, \n",
    "            vocab_size, \n",
    "            embed_dim, \n",
    "            num_heads, \n",
    "            ff_dim\n",
    "        ) # embed_dim\n",
    "        ## TextCNN部分：\n",
    "        self.proj_cnn = nn.Linear(ori_emb_dim, embed_dim)\n",
    "        self.textcnn = TextCNN(\n",
    "            embedding_dim = embed_dim, \n",
    "            kernel_sizes = tcnn_ks,\n",
    "            num_channels = tcnn_nc\n",
    "        ) # sum(tcnn_nc)\n",
    "        ## BiLSTM:\n",
    "        self.proj_lstm = nn.Linear(ori_emb_dim, embed_dim)\n",
    "        self.lstm = LSTMClassifier(\n",
    "            input_size = embed_dim, \n",
    "            hidden_size = lstm_hs, \n",
    "            num_layers = lstm_nlyr,  \n",
    "            bidirectional = lstm_bd\n",
    "        ) # lstm_hs * (2 if lstm_bd else 1)\n",
    "        self.mix = nn.Sequential(\n",
    "            ## 基于这个数字 embed_dim + sum(tcnn_nc) + lstm_hs * (2 if lstm_bd else 1) ，做一个全连接神经网络吧。\n",
    "            nn.Linear(\n",
    "                embed_dim + sum(tcnn_nc) + lstm_hs * (2 if lstm_bd else 1),\n",
    "                ff_dim\n",
    "            ),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.1),\n",
    "            nn.Linear(ff_dim, ff_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.1),\n",
    "            nn.Linear(ff_dim, type_of_class)\n",
    "        )\n",
    "        self.sc_net = SentimentClassifier(sc_input_dim, type_of_class)\n",
    "        \n",
    "    def forward(self, x, x_tfidf):\n",
    "        x_ori = x\n",
    "        x_emb = self.token_emb(x)\n",
    "        ## Tsfm部分：\n",
    "        x_tsfm = self.tsfm(self.proj_trans(x_emb)) \n",
    "        ## TextCNN部分：\n",
    "        x_tcnn = self.textcnn(self.proj_cnn(x_emb))\n",
    "        ## BiLSTM部分:\n",
    "        x_lstm = self.lstm(self.proj_lstm(x_emb))\n",
    "        ## 综合：\n",
    "        x_cat = torch.cat(\n",
    "            [\n",
    "                x_tsfm, \n",
    "                x_tcnn, x_lstm\n",
    "            ], axis=1\n",
    "        )        \n",
    "        return F.log_softmax(self.mix(x_cat) + self.sc_net(x_tfidf), dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc16d4c1-5977-45e9-8781-8648b42ba370",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5e34b472-f637-4c02-be8a-9cf3c4a61b25",
   "metadata": {},
   "source": [
    "## 构建模型以及训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "bc234083-f747-4d50-b535-6ba4703f6a9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = FinalModel(maxlen, vocab_size, embed_dim=128, num_heads=8, ff_dim=128)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=0.001)\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "b2f8b83d-9853-441b-8e13-c10c0596239f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FinalModel(\n",
       "  (token_emb): Embedding(8000, 512)\n",
       "  (proj_trans): Linear(in_features=512, out_features=128, bias=True)\n",
       "  (tsfm): TransformerModel(\n",
       "    (embedding_layer): TokenAndPositionEmbedding(\n",
       "      (pos_emb): Embedding(800, 128)\n",
       "    )\n",
       "    (transformer_block): TransformerBlock(\n",
       "      (att): MultiheadAttention(\n",
       "        (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)\n",
       "      )\n",
       "      (ffn): Sequential(\n",
       "        (0): Linear(in_features=128, out_features=128, bias=True)\n",
       "        (1): ReLU()\n",
       "        (2): Linear(in_features=128, out_features=128, bias=True)\n",
       "      )\n",
       "      (layernorm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "      (layernorm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "      (dropout1): Dropout(p=0.1, inplace=False)\n",
       "      (dropout2): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (global_avg_pool): AdaptiveAvgPool1d(output_size=1)\n",
       "  )\n",
       "  (proj_cnn): Linear(in_features=512, out_features=128, bias=True)\n",
       "  (textcnn): TextCNN(\n",
       "    (cnn_layers): ModuleList(\n",
       "      (0): Sequential(\n",
       "        (0): Conv1d(128, 32, kernel_size=(3,), stride=(1,))\n",
       "        (1): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU(inplace=True)\n",
       "      )\n",
       "      (1): Sequential(\n",
       "        (0): Conv1d(128, 64, kernel_size=(5,), stride=(1,))\n",
       "        (1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU(inplace=True)\n",
       "      )\n",
       "      (2): Sequential(\n",
       "        (0): Conv1d(128, 64, kernel_size=(7,), stride=(1,))\n",
       "        (1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU(inplace=True)\n",
       "      )\n",
       "      (3): Sequential(\n",
       "        (0): Conv1d(128, 64, kernel_size=(10,), stride=(1,))\n",
       "        (1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU(inplace=True)\n",
       "      )\n",
       "    )\n",
       "    (pool): GlobalMaxPool1d()\n",
       "  )\n",
       "  (proj_lstm): Linear(in_features=512, out_features=128, bias=True)\n",
       "  (lstm): LSTMClassifier(\n",
       "    (lstm): LSTM(128, 128, num_layers=4, batch_first=True, bidirectional=True)\n",
       "  )\n",
       "  (mix): Sequential(\n",
       "    (0): Linear(in_features=608, out_features=128, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Dropout(p=0.1, inplace=False)\n",
       "    (3): Linear(in_features=128, out_features=128, bias=True)\n",
       "    (4): ReLU()\n",
       "    (5): Dropout(p=0.1, inplace=False)\n",
       "    (6): Linear(in_features=128, out_features=14, bias=True)\n",
       "  )\n",
       "  (sc_net): SentimentClassifier(\n",
       "    (fc): Linear(in_features=6695, out_features=32, bias=True)\n",
       "    (dropout1): Dropout(p=0.2, inplace=False)\n",
       "    (dense1): Linear(in_features=32, out_features=32, bias=True)\n",
       "    (dropout2): Dropout(p=0.2, inplace=False)\n",
       "    (dense2): Linear(in_features=32, out_features=14, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "9a810569-9f27-4e37-bce5-8205405ddf49",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 训练和评估模型\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import shutil\n",
    "\n",
    "def train_model(model, train_loader, val_loader, criterion, optimizer, epochs=2):\n",
    "    # device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    model.to(device)\n",
    "    \n",
    "    log_path = \"runs/Logs\"\n",
    "    if os.path.exists(\n",
    "        os.path.dirname(log_path)\n",
    "    ):\n",
    "        shutil.rmtree(os.path.dirname(log_path))\n",
    "\n",
    "    writer = SummaryWriter(log_dir=log_path)\n",
    "    \n",
    "    total_train_step = 0\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        loss_sum = 0\n",
    "        for inputs, input_tfidfs, targets in tqdm.tqdm(train_loader):\n",
    "            inputs, input_tfidfs, targets = inputs.to(device), input_tfidfs.to(device), targets.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs, input_tfidfs)\n",
    "            loss = criterion(outputs, targets)\n",
    "            loss.requires_grad_(True)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_loss += loss.item()\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            total += targets.size(0)\n",
    "            correct += predicted.eq(targets).sum().item()\n",
    "            \n",
    "            total_train_step += 1\n",
    "            loss_sum += loss.item()\n",
    "            if total_train_step % 100 == 0:\n",
    "                # print(total_train_step//100)\n",
    "                writer.add_scalar(f\"train_loss_detail-{scheme_type_here}\", loss_sum/100, total_train_step//100)\n",
    "                loss_sum = 0\n",
    "                \n",
    "        print(f'Epoch {epoch + 1}/{epochs}, Loss: {train_loss/len(train_loader)}, Accuracy: {100.*correct/total}%')\n",
    "        writer.add_scalar(f\"epoch_loss-{scheme_type_here}\", train_loss/len(train_loader), epoch+1)\n",
    "        \n",
    "        model.eval()\n",
    "        val_loss = 0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        with torch.no_grad():\n",
    "            total_predicted = []\n",
    "            total_label = []\n",
    "            for inputs, input_tfidfs, targets in tqdm.tqdm(val_loader):\n",
    "                inputs, input_tfidfs, targets = inputs.to(device), input_tfidfs.to(device), targets.to(device)\n",
    "                outputs = model(inputs, input_tfidfs)\n",
    "                loss = criterion(outputs, targets)\n",
    "                val_loss += loss.item()\n",
    "                _, predicted = torch.max(outputs, 1)\n",
    "                total += targets.size(0)\n",
    "                correct += predicted.eq(targets).sum().item()\n",
    "\n",
    "                total_predicted += list(predicted.cpu())\n",
    "                total_label += list(targets.cpu())\n",
    "        f1 = f1_score(total_label, total_predicted, average='macro')\n",
    "                \n",
    "        print(f'Validation Loss: {val_loss/len(val_loader)}, Accuracy: {100.*correct/total}%, f1 score is {f1}')\n",
    "    writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "332e6466-7750-42b7-8503-47d9681b37ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████| 4375/4375 [04:30<00:00, 16.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15, Loss: 0.3740772521625672, Accuracy: 88.70214285714286%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████| 1875/1875 [00:42<00:00, 44.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 0.22246923875659705, Accuracy: 93.135%, f1 score is 0.9176475562128336\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████| 4375/4375 [04:29<00:00, 16.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/15, Loss: 0.21009888617928538, Accuracy: 93.66857142857143%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████| 1875/1875 [00:42<00:00, 44.05it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 0.20521633251508076, Accuracy: 93.795%, f1 score is 0.9243268199072087\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████| 4375/4375 [04:29<00:00, 16.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/15, Loss: 0.1737617210941123, Accuracy: 94.64714285714285%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████| 1875/1875 [00:42<00:00, 44.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 0.20258353450223804, Accuracy: 93.99833333333333%, f1 score is 0.9266895965302121\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████| 4375/4375 [04:29<00:00, 16.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/15, Loss: 0.14536175990280295, Accuracy: 95.53857142857143%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████| 1875/1875 [00:42<00:00, 44.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 0.1924387978874147, Accuracy: 94.18833333333333%, f1 score is 0.9280358862229895\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████| 4375/4375 [04:29<00:00, 16.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/15, Loss: 0.12350369844596301, Accuracy: 96.02785714285714%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████| 1875/1875 [00:42<00:00, 44.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 0.19282185274759928, Accuracy: 94.38833333333334%, f1 score is 0.9348072835427755\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████| 4375/4375 [04:29<00:00, 16.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/15, Loss: 0.10505266443258152, Accuracy: 96.63071428571429%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████| 1875/1875 [00:42<00:00, 44.05it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 0.20738096985661736, Accuracy: 94.18666666666667%, f1 score is 0.9303970876535498\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████| 4375/4375 [04:29<00:00, 16.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/15, Loss: 0.09142157223665022, Accuracy: 97.02428571428571%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████| 1875/1875 [00:42<00:00, 44.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 0.20044405336625254, Accuracy: 94.17666666666666%, f1 score is 0.9330439160772191\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████| 4375/4375 [04:29<00:00, 16.25it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/15, Loss: 0.07691480434419189, Accuracy: 97.42285714285714%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████| 1875/1875 [00:42<00:00, 44.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 0.2259684952680953, Accuracy: 94.29833333333333%, f1 score is 0.9337127426218473\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████| 4375/4375 [04:29<00:00, 16.25it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9/15, Loss: 0.06871094892332996, Accuracy: 97.70071428571428%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████| 1875/1875 [00:42<00:00, 44.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 0.22469598795470472, Accuracy: 94.28%, f1 score is 0.9327652180117952\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████| 4375/4375 [04:29<00:00, 16.25it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/15, Loss: 0.061109739370617484, Accuracy: 97.97928571428571%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████| 1875/1875 [00:42<00:00, 44.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 0.24996951154020305, Accuracy: 93.88333333333334%, f1 score is 0.9301360155054346\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████| 4375/4375 [04:29<00:00, 16.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11/15, Loss: 0.05355396345885321, Accuracy: 98.16214285714285%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████| 1875/1875 [00:42<00:00, 44.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 0.25289686508687836, Accuracy: 94.115%, f1 score is 0.9261031055217089\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 17%|█████████████▍                                                                  | 733/4375 [00:45<03:44, 16.23it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[39], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m15\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[38], line 33\u001b[0m, in \u001b[0;36mtrain_model\u001b[0;34m(model, train_loader, val_loader, criterion, optimizer, epochs)\u001b[0m\n\u001b[1;32m     31\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[1;32m     32\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[0;32m---> 33\u001b[0m train_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     34\u001b[0m _, predicted \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mmax(outputs, \u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     35\u001b[0m total \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m targets\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m0\u001b[39m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train_model(model, train_loader, val_loader, criterion, optimizer, epochs=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d86a3573-484f-4496-bd03-57cc32dc999f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "86ef1bb6-3766-4f32-9d3b-9062272ccc28",
   "metadata": {},
   "source": [
    "## 保存模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d527159d-b2b7-4ac9-8f8d-1bbe68c4852b",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_pickle_object(model, create_trained_models_path(f\"ori_9319-{scheme_type_here}.pkl\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ee913e4-d8e0-4869-8a50-bf1efad1391b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "963e8765-3268-475a-a055-d6180cbff2d4",
   "metadata": {},
   "source": [
    "# Scheme 3: 独立embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "50b718e7-72be-4895-9093-d0ce62bcd951",
   "metadata": {},
   "outputs": [],
   "source": [
    "scheme_type_here = f\"{scheme_type}__idp_emb\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "909540b1-a875-436c-9325-2e2929c675f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义模型\n",
    "class FinalModel(nn.Module):\n",
    "    def __init__(self, \n",
    "                 maxlen, vocab_size, \n",
    "                 embed_dim=128, \n",
    "                 num_heads=8, ff_dim=128,\n",
    "                tcnn_ks = [3,5,7,10], tcnn_nc = [32,64,64,64],\n",
    "                 lstm_hs = 128, lstm_nlyr = 4, lstm_bd = True\n",
    "                ):\n",
    "        super(FinalModel, self).__init__()\n",
    "        ## Tsfm部分：\n",
    "        self.emb_trans = nn.Embedding(vocab_size, embed_dim)\n",
    "        self.tsfm = TransformerModel(\n",
    "            maxlen, \n",
    "            vocab_size, \n",
    "            embed_dim, \n",
    "            num_heads, \n",
    "            ff_dim\n",
    "        ) # embed_dim\n",
    "        ## TextCNN部分：\n",
    "        self.emb_cnn = nn.Embedding(vocab_size, embed_dim)\n",
    "        self.textcnn = TextCNN(\n",
    "            embedding_dim = embed_dim, \n",
    "            kernel_sizes = tcnn_ks,\n",
    "            num_channels = tcnn_nc\n",
    "        ) # sum(tcnn_nc)\n",
    "        ## BiLSTM:\n",
    "        self.emb_lstm = nn.Embedding(vocab_size, embed_dim)\n",
    "        self.lstm = LSTMClassifier(\n",
    "            input_size = embed_dim, \n",
    "            hidden_size = lstm_hs, \n",
    "            num_layers = lstm_nlyr,  \n",
    "            bidirectional = lstm_bd\n",
    "        ) # lstm_hs * (2 if lstm_bd else 1)\n",
    "        self.mix = nn.Sequential(\n",
    "            ## 基于这个数字 embed_dim + sum(tcnn_nc) + lstm_hs * (2 if lstm_bd else 1) ，做一个全连接神经网络吧。\n",
    "            nn.Linear(\n",
    "                embed_dim + sum(tcnn_nc) + lstm_hs * (2 if lstm_bd else 1),\n",
    "                ff_dim\n",
    "            ),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.1),\n",
    "            nn.Linear(ff_dim, ff_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.1),\n",
    "            nn.Linear(ff_dim, type_of_class)\n",
    "        )\n",
    "        self.sc_net = SentimentClassifier(sc_input_dim, type_of_class)\n",
    "        \n",
    "    def forward(self, x, x_tfidf):\n",
    "        x_ori = x\n",
    "        ## Tsfm部分：\n",
    "        x_tsfm = self.tsfm(self.emb_trans(x_ori)) \n",
    "        ## TextCNN部分：\n",
    "        x_tcnn = self.textcnn(self.emb_cnn(x_ori))\n",
    "        ## BiLSTM部分:\n",
    "        x_lstm = self.lstm(self.emb_lstm(x_ori))\n",
    "        ## 综合：\n",
    "        x_cat = torch.cat(\n",
    "            [\n",
    "                x_tsfm, \n",
    "                x_tcnn, x_lstm\n",
    "            ], axis=1\n",
    "        )        \n",
    "        return F.log_softmax(self.mix(x_cat) + self.sc_net(x_tfidf), dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6f09bfd-31e5-4db7-89ac-1fda0c7efd2d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "40f5cd96-5cb0-4b33-9d71-b7c3d01fad96",
   "metadata": {},
   "source": [
    "## 构建模型以及训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "b3873c12-c384-4ee2-8cea-34c468dbcce8",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = FinalModel(maxlen, vocab_size, embed_dim=128, num_heads=8, ff_dim=128)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=0.001)\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "24f8a426-e3ec-4216-ad50-cfedacae61bd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FinalModel(\n",
       "  (emb_trans): Embedding(8000, 128)\n",
       "  (tsfm): TransformerModel(\n",
       "    (embedding_layer): TokenAndPositionEmbedding(\n",
       "      (pos_emb): Embedding(800, 128)\n",
       "    )\n",
       "    (transformer_block): TransformerBlock(\n",
       "      (att): MultiheadAttention(\n",
       "        (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)\n",
       "      )\n",
       "      (ffn): Sequential(\n",
       "        (0): Linear(in_features=128, out_features=128, bias=True)\n",
       "        (1): ReLU()\n",
       "        (2): Linear(in_features=128, out_features=128, bias=True)\n",
       "      )\n",
       "      (layernorm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "      (layernorm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "      (dropout1): Dropout(p=0.1, inplace=False)\n",
       "      (dropout2): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (global_avg_pool): AdaptiveAvgPool1d(output_size=1)\n",
       "  )\n",
       "  (emb_cnn): Embedding(8000, 128)\n",
       "  (textcnn): TextCNN(\n",
       "    (cnn_layers): ModuleList(\n",
       "      (0): Sequential(\n",
       "        (0): Conv1d(128, 32, kernel_size=(3,), stride=(1,))\n",
       "        (1): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU(inplace=True)\n",
       "      )\n",
       "      (1): Sequential(\n",
       "        (0): Conv1d(128, 64, kernel_size=(5,), stride=(1,))\n",
       "        (1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU(inplace=True)\n",
       "      )\n",
       "      (2): Sequential(\n",
       "        (0): Conv1d(128, 64, kernel_size=(7,), stride=(1,))\n",
       "        (1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU(inplace=True)\n",
       "      )\n",
       "      (3): Sequential(\n",
       "        (0): Conv1d(128, 64, kernel_size=(10,), stride=(1,))\n",
       "        (1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU(inplace=True)\n",
       "      )\n",
       "    )\n",
       "    (pool): GlobalMaxPool1d()\n",
       "  )\n",
       "  (emb_lstm): Embedding(8000, 128)\n",
       "  (lstm): LSTMClassifier(\n",
       "    (lstm): LSTM(128, 128, num_layers=4, batch_first=True, bidirectional=True)\n",
       "  )\n",
       "  (mix): Sequential(\n",
       "    (0): Linear(in_features=608, out_features=128, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Dropout(p=0.1, inplace=False)\n",
       "    (3): Linear(in_features=128, out_features=128, bias=True)\n",
       "    (4): ReLU()\n",
       "    (5): Dropout(p=0.1, inplace=False)\n",
       "    (6): Linear(in_features=128, out_features=14, bias=True)\n",
       "  )\n",
       "  (sc_net): SentimentClassifier(\n",
       "    (fc): Linear(in_features=6695, out_features=32, bias=True)\n",
       "    (dropout1): Dropout(p=0.2, inplace=False)\n",
       "    (dense1): Linear(in_features=32, out_features=32, bias=True)\n",
       "    (dropout2): Dropout(p=0.2, inplace=False)\n",
       "    (dense2): Linear(in_features=32, out_features=14, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "4b1a533d-aef6-40ae-bd7f-f3f028b5ea76",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 训练和评估模型\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import shutil\n",
    "\n",
    "def train_model(model, train_loader, val_loader, criterion, optimizer, epochs=2):\n",
    "    # device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    model.to(device)\n",
    "    \n",
    "    log_path = \"runs/Logs\"\n",
    "    if os.path.exists(\n",
    "        os.path.dirname(log_path)\n",
    "    ):\n",
    "        shutil.rmtree(os.path.dirname(log_path))\n",
    "\n",
    "    writer = SummaryWriter(log_dir=log_path)\n",
    "    \n",
    "    total_train_step = 0\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        loss_sum = 0\n",
    "        for inputs, input_tfidfs, targets in tqdm.tqdm(train_loader):\n",
    "            inputs, input_tfidfs, targets = inputs.to(device), input_tfidfs.to(device), targets.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs, input_tfidfs)\n",
    "            loss = criterion(outputs, targets)\n",
    "            loss.requires_grad_(True)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_loss += loss.item()\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            total += targets.size(0)\n",
    "            correct += predicted.eq(targets).sum().item()\n",
    "            \n",
    "            total_train_step += 1\n",
    "            loss_sum += loss.item()\n",
    "            if total_train_step % 100 == 0:\n",
    "                # print(total_train_step//100)\n",
    "                writer.add_scalar(f\"train_loss_detail-{scheme_type_here}\", loss_sum/100, total_train_step//100)\n",
    "                loss_sum = 0\n",
    "                \n",
    "        print(f'Epoch {epoch + 1}/{epochs}, Loss: {train_loss/len(train_loader)}, Accuracy: {100.*correct/total}%')\n",
    "        writer.add_scalar(f\"epoch_loss-{scheme_type_here}\", train_loss/len(train_loader), epoch+1)\n",
    "        \n",
    "        model.eval()\n",
    "        val_loss = 0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        with torch.no_grad():\n",
    "            total_predicted = []\n",
    "            total_label = []\n",
    "            for inputs, input_tfidfs, targets in tqdm.tqdm(val_loader):\n",
    "                inputs, input_tfidfs, targets = inputs.to(device), input_tfidfs.to(device), targets.to(device)\n",
    "                outputs = model(inputs, input_tfidfs)\n",
    "                loss = criterion(outputs, targets)\n",
    "                val_loss += loss.item()\n",
    "                _, predicted = torch.max(outputs, 1)\n",
    "                total += targets.size(0)\n",
    "                correct += predicted.eq(targets).sum().item()\n",
    "\n",
    "                total_predicted += list(predicted.cpu())\n",
    "                total_label += list(targets.cpu())\n",
    "        f1 = f1_score(total_label, total_predicted, average='macro')\n",
    "                \n",
    "        print(f'Validation Loss: {val_loss/len(val_loader)}, Accuracy: {100.*correct/total}%, f1 score is {f1}')\n",
    "    writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "24a43d54-8070-4f29-b0a6-c76002d5393b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 77%|████████████████████████████████████████████████████████████▋                  | 3361/4375 [03:21<01:00, 16.65it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[45], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m15\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[44], line 33\u001b[0m, in \u001b[0;36mtrain_model\u001b[0;34m(model, train_loader, val_loader, criterion, optimizer, epochs)\u001b[0m\n\u001b[1;32m     31\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[1;32m     32\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[0;32m---> 33\u001b[0m train_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     34\u001b[0m _, predicted \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mmax(outputs, \u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     35\u001b[0m total \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m targets\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m0\u001b[39m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train_model(model, train_loader, val_loader, criterion, optimizer, epochs=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d64b4ef2-1aaa-4413-912e-0fd897844b03",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "677a4fc9-c8b5-4db3-ac2d-7f2eaff36386",
   "metadata": {},
   "source": [
    "## 保存模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dae26e19-9cc4-4fee-9941-663749b7c1e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_pickle_object(model, create_trained_models_path(f\"ori_9319-{scheme_type_here}.pkl\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95aa8524-16ac-41b6-a19c-abc91aedbb26",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a37a979-25cf-4f44-bfa8-95763ca7f745",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e28f73fe-c05f-45fb-ac86-437d8adad664",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# 选最好的模型来预测"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f2f76139-ebd1-49a1-98c9-faf5655054f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "scheme_type_best = \"TBD\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "f419efd0-8862-48ce-814f-1fae5bd1641b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = load_pickle_object(create_trained_models_path(f\"ori_9319-{scheme_type_best}.pkl\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "0a123afe-2fe4-4f8a-903b-1b24f4810a7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████| 32/32 [00:01<00:00, 25.93it/s]\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    total_predicted = []\n",
    "    for inputs, input_tfidfs, targets in tqdm.tqdm(test_loader):\n",
    "        inputs, input_tfidfs, targets = inputs.to(device), input_tfidfs.to(device), targets.to(device)\n",
    "        outputs = model(inputs, input_tfidfs)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        total_predicted += list(predicted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "706d090f-a3bc-45f4-81f2-e85fef155296",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1000"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "oot_rst = [int(x) for x in total_predicted]\n",
    "len(oot_rst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "b7bd7c9e-be76-4c3e-89c7-beae8a4ccf90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df.to_csv(\"/Users/minkexiu/Downloads/GitHub/Tianchi_NLPNewsClassification/preprocessedData/rst-20250217_1.csv\", index=False)\n",
      "data saved.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'/Users/minkexiu/Downloads/GitHub/Tianchi_NLPNewsClassification/preprocessedData/rst-20250217_1.csv'"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "store_data_to_newbasepath(pd.DataFrame({\"label\": oot_rst}), f\"rst-{scheme_type_best}\", fmt=\"csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3db65fc-dd7b-4ca6-a420-bbe43e95b9b6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
